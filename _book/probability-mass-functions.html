<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 3 Probability Mass Functions | Think Stats</title>
  <meta name="description" content="Version 2.0.35">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 3 Probability Mass Functions | Think Stats" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Version 2.0.35" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Probability Mass Functions | Think Stats" />
  
  <meta name="twitter:description" content="Version 2.0.35" />
  

<meta name="author" content="Allen B. Downey">


<meta name="date" content="2019-03-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="distributions.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#how-i-wrote-this-book"><i class="fa fa-check"></i><b>0.1</b> How I wrote this book</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#using-the-code"><i class="fa fa-check"></i><b>0.2</b> Using the code</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#contributor-list"><i class="fa fa-check"></i><b>0.3</b> Contributor List</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="exploratory.html"><a href="exploratory.html"><i class="fa fa-check"></i><b>1</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="1.1" data-path="exploratory.html"><a href="exploratory.html#a-statistical-approach"><i class="fa fa-check"></i><b>1.1</b> A Statistical Approach</a></li>
<li class="chapter" data-level="1.2" data-path="exploratory.html"><a href="exploratory.html#the-national-survey-of-family-growth"><i class="fa fa-check"></i><b>1.2</b> The National Survey of Family Growth</a></li>
<li class="chapter" data-level="1.3" data-path="exploratory.html"><a href="exploratory.html#importing-the-data"><i class="fa fa-check"></i><b>1.3</b> Importing the Data</a></li>
<li class="chapter" data-level="1.4" data-path="exploratory.html"><a href="exploratory.html#dataframes"><i class="fa fa-check"></i><b>1.4</b> DataFrames</a></li>
<li class="chapter" data-level="1.5" data-path="exploratory.html"><a href="exploratory.html#variables"><i class="fa fa-check"></i><b>1.5</b> Variables</a></li>
<li class="chapter" data-level="1.6" data-path="exploratory.html"><a href="exploratory.html#transformation"><i class="fa fa-check"></i><b>1.6</b> Transformation</a></li>
<li class="chapter" data-level="1.7" data-path="exploratory.html"><a href="exploratory.html#validation"><i class="fa fa-check"></i><b>1.7</b> Validation</a></li>
<li class="chapter" data-level="1.8" data-path="exploratory.html"><a href="exploratory.html#interpretation"><i class="fa fa-check"></i><b>1.8</b> Interpretation</a></li>
<li class="chapter" data-level="1.9" data-path="exploratory.html"><a href="exploratory.html#exercises"><i class="fa fa-check"></i><b>1.9</b> Exercises</a></li>
<li class="chapter" data-level="1.10" data-path="exploratory.html"><a href="exploratory.html#glossary"><i class="fa fa-check"></i><b>1.10</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>2</b> Distributions</a><ul>
<li class="chapter" data-level="2.1" data-path="distributions.html"><a href="distributions.html#histograms"><i class="fa fa-check"></i><b>2.1</b> Histograms</a></li>
<li class="chapter" data-level="2.2" data-path="distributions.html"><a href="distributions.html#representing-histograms"><i class="fa fa-check"></i><b>2.2</b> Representing Histograms</a></li>
<li class="chapter" data-level="2.3" data-path="distributions.html"><a href="distributions.html#plotting-histograms"><i class="fa fa-check"></i><b>2.3</b> Plotting Histograms</a></li>
<li class="chapter" data-level="2.4" data-path="distributions.html"><a href="distributions.html#nsfg-variables"><i class="fa fa-check"></i><b>2.4</b> NSFG Variables</a></li>
<li class="chapter" data-level="2.5" data-path="distributions.html"><a href="distributions.html#outliers"><i class="fa fa-check"></i><b>2.5</b> Outliers</a></li>
<li class="chapter" data-level="2.6" data-path="distributions.html"><a href="distributions.html#first-babies"><i class="fa fa-check"></i><b>2.6</b> First Babies</a></li>
<li class="chapter" data-level="2.7" data-path="distributions.html"><a href="distributions.html#summarizing-distributions"><i class="fa fa-check"></i><b>2.7</b> Summarizing Distributions</a></li>
<li class="chapter" data-level="2.8" data-path="distributions.html"><a href="distributions.html#variance"><i class="fa fa-check"></i><b>2.8</b> Variance</a></li>
<li class="chapter" data-level="2.9" data-path="distributions.html"><a href="distributions.html#effect-size"><i class="fa fa-check"></i><b>2.9</b> Effect Size</a></li>
<li class="chapter" data-level="2.10" data-path="distributions.html"><a href="distributions.html#reporting-results"><i class="fa fa-check"></i><b>2.10</b> Reporting Results</a></li>
<li class="chapter" data-level="2.11" data-path="distributions.html"><a href="distributions.html#exercises-1"><i class="fa fa-check"></i><b>2.11</b> Exercises</a></li>
<li class="chapter" data-level="2.12" data-path="distributions.html"><a href="distributions.html#glossary-1"><i class="fa fa-check"></i><b>2.12</b> Glossary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html"><i class="fa fa-check"></i><b>3</b> Probability Mass Functions</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html#pmfs"><i class="fa fa-check"></i><b>3.1</b> PMFs</a></li>
<li class="chapter" data-level="3.2" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html#plotting-pmfs"><i class="fa fa-check"></i><b>3.2</b> Plotting PMFs</a></li>
<li class="chapter" data-level="3.3" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html#other-visualizations"><i class="fa fa-check"></i><b>3.3</b> Other Visualizations</a></li>
<li class="chapter" data-level="3.4" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html#the-class-size-paradox"><i class="fa fa-check"></i><b>3.4</b> The Class Size Paradox</a></li>
<li class="chapter" data-level="3.5" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html#dataframe-indexing"><i class="fa fa-check"></i><b>3.5</b> DataFrame Indexing</a></li>
<li class="chapter" data-level="3.6" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html#exercises-2"><i class="fa fa-check"></i><b>3.6</b> Exercises</a></li>
<li class="chapter" data-level="3.7" data-path="probability-mass-functions.html"><a href="probability-mass-functions.html#glossary-2"><i class="fa fa-check"></i><b>3.7</b> Glossary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Think Stats</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability-mass-functions" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Probability Mass Functions</h1>
<p>The code for this chapter is in <code>probability.py</code>. For information
about downloading and working with this code, see Section <a href="index.html#using-the-code">0.2</a>.</p>
<div id="pmfs" class="section level2">
<h2><span class="header-section-number">3.1</span> PMFs</h2>
<p>Another way to represent a distribution
is a <strong>probability mass function</strong>
(PMF), which maps from each value to its probability. A <strong>probability</strong> is a frequency expressed as
a fraction of the sample size, <code>n</code>. To get from frequencies to
probabilities, we divide through by <code>n</code>, which is called <strong>normalization</strong>.</p>
<p>Given a <code>Hist</code>, we can make a dictionary
that maps from each value to its probability:</p>
<pre><code>n = hist.Total()
d = {}
for x, freq in hist.Items():
    d[x] = freq / n</code></pre>
<p>Or we can use the <code>Pmf</code> class provided by
<code>thinkstats2</code>. Like <code>Hist</code>, the
<code>Pmf</code> constructor can take a list, pandas Series, dictionary, Hist, or
another Pmf object. Here’s an example with a simple list:</p>
<pre><code>&gt;&gt;&gt; import thinkstats2
&gt;&gt;&gt; pmf = thinkstats2.Pmf([1, 2, 2, 3, 5])
&gt;&gt;&gt; pmf
Pmf({1: 0.2, 2: 0.4, 3: 0.2, 5: 0.2})</code></pre>
<p>The <code>Pmf</code> is normalized so total probability is 1.</p>
<p><code>Pmf</code> and <code>Hist</code> objects are similar in many
ways; in fact, they inherit many of their methods from a common parent
class. For example, the methods <code>Values</code> and <code>Items</code> work the same way for both.
The biggest difference is that a <code>Hist</code> maps from values to integer
counters; a <code>Pmf</code> maps from values to floating-point probabilities.</p>
<p>To look up the probability associated
with a value, use <code>Prob</code>:</p>
<pre><code>&gt;&gt;&gt; pmf.Prob(2)
0.4</code></pre>
<p>The bracket operator is equivalent:</p>
<pre><code>&gt;&gt;&gt; pmf[2]
0.4</code></pre>
<p>You can modify an existing <code>Pmf</code> by
incrementing the probability associated with a value:</p>
<pre><code>&gt;&gt;&gt; pmf.Incr(2, 0.2)
&gt;&gt;&gt; pmf.Prob(2)
0.6</code></pre>
<p>Or you can multiply a probability by a
factor:</p>
<pre><code>&gt;&gt;&gt; pmf.Mult(2, 0.5)
&gt;&gt;&gt; pmf.Prob(2)
0.3</code></pre>
<p>If you modify a <code>Pmf</code>, the result may not
be normalized; that is, the probabilities may no longer add up to 1. To
check, you can call <code>Total</code>,
which returns the sum of the probabilities:</p>
<pre><code>&gt;&gt;&gt; pmf.Total()
0.9</code></pre>
<p>To renormalize, call <code>Normalize</code>:</p>
<pre><code>&gt;&gt;&gt; pmf.Normalize()
&gt;&gt;&gt; pmf.Total()
1.0</code></pre>
<p><code>Pmf</code> objects provide a <code>Copy</code> method so you can make and
modify a copy without affecting the original.</p>
<p>My notation in this section might seem
inconsistent, but there is a system: I use <code>Pmf</code> for the name of the
class, <code>pmf</code> for an instance of
the class, and PMF for the mathematical concept of a probability mass
function.</p>
</div>
<div id="plotting-pmfs" class="section level2">
<h2><span class="header-section-number">3.2</span> Plotting PMFs</h2>
<p><code>thinkplot</code> provides two ways to plot Pmfs:</p>
<ul>
<li>To plot a Pmf as a bar graph, you can
use <code>thinkplot.Hist</code>. Bar
graphs are most useful if the number of values in the Pmf is small.</li>
<li>To plot a Pmf as a step function, you
can use <code>thinkplot.Pmf</code>.
This option is most useful if there are a large number of values and
the Pmf is smooth. This function also works with Hist objects.</li>
</ul>
<p>In addition, <code>pyplot</code> provides a function called
<code>hist</code> that takes a sequence of
values, computes a histogram, and plots it. Since I use <code>Hist</code> objects, I
usually don’t use <code>pyplot.hist</code>.</p>
<div class="figure" style="text-align: center"><span id="fig:probability-nsfg-pmf"></span>
<img src="images/06.png" alt="PMF of pregnancy lengths for first babies and others, using bar graphs and step functions." width="90%" />
<p class="caption">
Figure 3.1: PMF of pregnancy lengths for first babies and others, using bar graphs and step functions.
</p>
</div>
<p>Figure <a href="#probability-nsfg-pmf">3.1</a> shows PMFs of pregnancy length for first
babies and others using bar graphs (left) and step functions (right).</p>
<p>By plotting the PMF instead of the
histogram, we can compare the two distributions without being mislead by
the difference in sample size. Based on this figure, first babies seem
to be less likely than others to arrive on time (week 39) and more
likely to be a late (weeks 41 and 42).</p>
<p>Here’s the code that generates
Figure <a href="#probability-nsfg-pmf">3.1</a></p>
<pre><code>    thinkplot.PrePlot(2, cols=2)
    thinkplot.Hist(first_pmf, align=&#39;right&#39;, width=width)
    thinkplot.Hist(other_pmf, align=&#39;left&#39;, width=width)
    thinkplot.Config(xlabel=&#39;weeks&#39;,
                     ylabel=&#39;probability&#39;,
                     axis=[27, 46, 0, 0.6])

    thinkplot.PrePlot(2)
    thinkplot.SubPlot(2)
    thinkplot.Pmfs([first_pmf, other_pmf])
    thinkplot.Show(xlabel=&#39;weeks&#39;,
                   axis=[27, 46, 0, 0.6])</code></pre>
<p><code>PrePlot</code> takes optional parameters
<code>rows</code> and <code>cols</code> to make a grid of figures, in
this case one row of two figures. The first figure (on the left)
displays the Pmfs using <code>thinkplot.Hist</code>, as we have seen
before.</p>
<p>The second call to <code>PrePlot</code> resets the color generator.
Then <code>SubPlot</code> switches to the
second figure (on the right) and displays the Pmfs using <code>thinkplot.Pmfs</code>. I used the <code>axis</code> option to
ensure that the two figures are on the same axes, which is generally a good idea if you
intend to compare two figures.</p>
</div>
<div id="other-visualizations" class="section level2">
<h2><span class="header-section-number">3.3</span> Other Visualizations</h2>
<p>Histograms and PMFs are useful while you
are exploring data and trying to identify patterns and relationships.
Once you have an idea what is going on, a good next step is to design a
visualization that makes the patterns you have identified as clear as
possible.</p>
<p>In the NSFG data, the biggest differences
in the distributions are near the mode. So it makes sense to zoom in on
that part of the graph, and to transform the data to emphasize
differences:</p>
<pre><code>    weeks = range(35, 46)
    diffs = []
    for week in weeks:
        p1 = first_pmf.Prob(week)
        p2 = other_pmf.Prob(week)
        diff = 100 * (p1 - p2)
        diffs.append(diff)

    thinkplot.Bar(weeks, diffs)</code></pre>
<p>In this code, <code>weeks</code> is the range of weeks; <code>diffs</code> is the difference between the
two PMFs in percentage points. Figure <a href="#probability-nsfg-diffs">3.2</a> shows the result as a bar chart. This figure
makes the pattern clearer: first babies are less likely to be born in
week 39, and somewhat more likely to be born in weeks 41 and 42.</p>
<div class="figure" style="text-align: center"><span id="fig:probability-nsfg-diffs"></span>
<img src="images/07.png" alt="Difference, in percentage points, by week." width="90%" />
<p class="caption">
Figure 3.2: Difference, in percentage points, by week.
</p>
</div>
<p>For now we should hold this conclusion
only tentatively. We used the same dataset to identify an apparent
difference and then chose a visualization that makes the difference
apparent. We can’t be sure this effect is real; it might be due to
random variation. We’ll address this concern later.</p>
</div>
<div id="the-class-size-paradox" class="section level2">
<h2><span class="header-section-number">3.4</span> The Class Size Paradox</h2>
<p>Before we go on, I want to demonstrate
one kind of computation you can do with <code>Pmf</code> objects; I call this example
the “class size paradox.”</p>
<p>At many American colleges and
universities, the student-to-faculty ratio is about 10:1. But students
are often surprised to discover that their average class size is bigger
than 10. There are two reasons for the discrepancy:</p>
<ul>
<li>Students typically take 4–5 classes
per semester, but professors often teach 1 or 2.</li>
<li>The number of students who enjoy a
small class is small, but the number of students in a large class is
(ahem!) large.</li>
</ul>
<p>The first effect is obvious, at least
once it is pointed out; the second is more subtle. Let’s look at an
example. Suppose that a college offers 65 classes in a given semester,
with the following distribution of sizes:</p>
<pre><code> size      count
 5- 9          8
10-14          8
15-19         14
20-24          4
25-29          6
30-34         12
35-39          8
40-44          3
45-49          2</code></pre>
<p>If you ask the Dean for the average class
size, he would construct a PMF, compute the mean, and report that the
average class size is 23.7. Here’s the code:</p>
<pre><code>    d = { 7: 8, 12: 8, 17: 14, 22: 4, 
          27: 6, 32: 12, 37: 8, 42: 3, 47: 2 }

    pmf = thinkstats2.Pmf(d, label=&#39;actual&#39;)
    print(&#39;mean&#39;, pmf.Mean())</code></pre>
<p>But if you survey a group of students,
ask them how many students are in their classes, and compute the mean,
you would think the average class was bigger. Let’s see how much
bigger.</p>
<p>First, I compute the distribution as
observed by students, where the probability associated with each class
size is “biased” by the number of students in the class.</p>
<pre><code>def BiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, x)
        
    new_pmf.Normalize()
    return new_pmf</code></pre>
<p>For each class size, <code>x</code>, we multiply the probability by
<code>x</code>, the number of students who
observe that class size. The result is a new Pmf that represents the
biased distribution.</p>
<p>Now we can plot the actual and observed
distributions:</p>
<pre><code>    biased_pmf = BiasPmf(pmf, label=&#39;observed&#39;)
    thinkplot.PrePlot(2)
    thinkplot.Pmfs([pmf, biased_pmf])
    thinkplot.Show(xlabel=&#39;class size&#39;, ylabel=&#39;PMF&#39;)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:class-size1"></span>
<img src="images/08.png" alt="Distribution of class sizes, actual and as observed by students." width="90%" />
<p class="caption">
Figure 3.3: Distribution of class sizes, actual and as observed by students.
</p>
</div>
<p>Figure <a href="#class-size1">3.3</a> shows the result. In the biased distribution
there are fewer small classes and more large ones. The mean of the
biased distribution is 29.1, almost 25% higher than the actual
mean.</p>
<p>It is also possible to invert this
operation. Suppose you want to find the distribution of class sizes at a
college, but you can’t get reliable data from the Dean. An alternative
is to choose a random sample of students and ask how many students are
in their classes.</p>
<p>The result would be biased for the
reasons we’ve just seen, but you can use it to estimate the actual
distribution. Here’s the function that unbiases a Pmf:</span></p>
<pre><code>def UnbiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, 1.0/x)
        
    new_pmf.Normalize()
    return new_pmf</code></pre>
<p>It’s similar to <code>BiasPmf</code>; the only difference is
that it divides each probability by <code>x</code> instead of multiplying.</p>
</div>
<div id="dataframe-indexing" class="section level2">
<h2><span class="header-section-number">3.5</span> DataFrame Indexing</h2>
<p>In Section <a href="exploratory.html#dataframes">1.4</a> we read a pandas DataFrame and used it to
select and modify data columns. Now let’s look at row selection. To
start, I create a NumPy array of random numbers and use it to initialize
a DataFrame:</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas
&gt;&gt;&gt; array = np.random.randn(4, 2)
&gt;&gt;&gt; df = pandas.DataFrame(array)
&gt;&gt;&gt; df
          0         1
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897</code></pre>
<p>By default, the rows and columns are
numbered starting at zero, but you can provide column names:</p>
<pre><code>&gt;&gt;&gt; columns = [&#39;A&#39;, &#39;B&#39;]
&gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns)
&gt;&gt;&gt; df
          A         B
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897</code></pre>
<p>You can also provide row names. The set
of row names is called the <strong>index</strong>
the row names themselves are called <strong>labels</strong>.</p>
<pre><code>&gt;&gt;&gt; index = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]
&gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns, index=index)
&gt;&gt;&gt; df
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621
d -1.369968  0.545897</code></pre>
<p>As we saw in the previous chapter, simple
indexing selects a column, returning a Series:</p>
<pre><code>&gt;&gt;&gt; df[&#39;A&#39;]
a   -0.143510
b   -1.489647
c   -0.074350
d   -1.369968
Name: A, dtype: float64</code></pre>
<p>To select a row by label, you can use the
<code>loc</code> attribute, which returns
a Series:</p>
<pre><code>&gt;&gt;&gt; df.loc[&#39;a&#39;]
A   -0.14351
B    0.61605
Name: a, dtype: float64</code></pre>
<p>If you know the integer position of a
row, rather than its label, you can use the <code>iloc</code> attribute, which also returns
a Series.</p>
<pre><code>&gt;&gt;&gt; df.iloc[0]
A   -0.14351
B    0.61605
Name: a, dtype: float64</code></pre>
<p><code>loc</code> can also take a list of labels;
in that case, the result is a DataFrame.</p>
<pre><code>&gt;&gt;&gt; indices = [&#39;a&#39;, &#39;c&#39;]
&gt;&gt;&gt; df.loc[indices]
         A         B
a -0.14351  0.616050
c -0.07435  0.039621</code></pre>
<p>Finally, you can use a slice to select a
range of rows by label:</p>
<pre><code>&gt;&gt;&gt; df[&#39;a&#39;:&#39;c&#39;]
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621</code></pre>
<p>Or by integer position:</p>
<pre><code>&gt;&gt;&gt; df[0:2]
          A         B
a -0.143510  0.616050
b -1.489647  0.300774</code></pre>
<p>The result in either case is a DataFrame,
but notice that the first result includes the end of the slice; the
second doesn’t.</p>
<p>My advice: if your rows have labels that
are not simple integers, use the labels consistently and avoid using
integer positions.</p>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">3.6</span> Exercises</h2>
<p>Solutions to these exercises are in
<code>chap03soln.ipynb</code> and <code>chap03soln.py</code></p>
<p><strong>Exercise 1</strong></p>
<p>Something like the class
size paradox appears if you survey children and ask how many children
are in their family. Families with many children are more likely to
appear in your sample, and families with no children have no chance to
be in the sample.</p>
<p>Use the NSFG respondent variable
<code>NUMKDHH</code> to construct the actual distribution for the number of
children under 18 in the household.</p>
<p>Now compute the biased distribution we
would see if we surveyed the children and asked them how many children
under 18 (including themselves) are in their household.</p>
<p>Plot the actual and biased
distributions, and compute their means. As a starting place, you can use
<code>chap03ex.ipynb</code>.</p>
<p><strong>Exercise 2</strong></p>
<p>In Section <a href="distributions.html#summarizing-distributions">2.7</a> we computed the mean of a sample by adding up
the elements and dividing by n. If you are given a PMF, you can still
compute the mean, but the process is slightly different:</p>
<p><span class="math display">\[
\bar x = \sum_i p_ix_i
\]</span></p>
<p><em>where the</em> <span class="math inline">\(x_i\)</span> <em>are the unique values in the PMF and</em> $p_i=PMF(x_i). Similarly, you can compute variance like
this:</p>
<p><span class="math display">\[
S^2 = \sum_i p_i (x_i - \bar x)^2
\]</span></p>
<p>Write functions called <code>PmfMean</code> and <code>PmfVar</code> that take a Pmf object and
compute the mean and variance. To test these methods, check that they
are consistent with the methods <code>Mean</code> and <code>Var</code> provided by Pmf.</p>
<p><strong>Exercise 3</strong></p>
<p>I started with the
question, “Are first babies more likely to be late?” To address it, I
computed the difference in means between groups of babies, but I ignored
the possibility that there might be a difference between first babies
and others for the same woman.</p>
<p>To address this version of the question,
select respondents who have at least two babies and compute pairwise
differences. Does this formulation of the question yield a different
result?</p>
<p><em>Hint: use <code>nsfg.MakePregMap</code>.</em></p>
<p><strong>Exercise 4</strong></p>
<p>In most foot races, everyone starts at
the same time. If you are a fast runner, you usually pass a lot of
people at the beginning of the race, but after a few miles everyone
around you is going at the same speed.</p>
<p>When I ran a long-distance (209 miles)
relay race for the first time, I noticed an odd phenomenon: when I
overtook another runner, I was usually much faster, and when another
runner overtook me, he was usually much faster.</p>
<p>At first I thought that the distribution
of speeds might be bimodal; that is, there were many slow runners and
many fast runners, but few at my speed.</p>
<p>Then I realized that I was the victim of
a bias similar to the effect of class size. The race was unusual in two
ways: it used a staggered start, so teams started at different times;
also, many teams included runners at different levels of ability.</p>
<p>As a result, runners were spread out
along the course with little relationship between speed and location.
When I joined the race, the runners near me were (pretty much) a random
sample of the runners in the race.</p>
<p>So where does the bias come from? During
my time on the course, the chance of overtaking a runner, or being
overtaken, is proportional to the difference in our speeds. I am more
likely to catch a slow runner, and more likely to be caught by a fast
runner. But runners at the same speed are unlikely to see each
other.</p>
<p>Write a function called <code>ObservedPmf</code> that takes a Pmf
representing the actual distribution of runners’ speeds, and the speed
of a running observer, and returns a new Pmf representing the
distribution of runners’ speeds as seen by the observer.</p>
<p>To test your function, you can use <code>relay.py</code>, which reads the results
from the James Joyce Ramble 10K in Dedham MA and converts the pace of
each runner to mph.</p>
<p>Compute the distribution of speeds you
would observe if you ran a relay race at 7.5 mph with this group of
runners. A solution to this exercise is in <code>relay_soln.py</code>.</p>
</div>
<div id="glossary-2" class="section level2">
<h2><span class="header-section-number">3.7</span> Glossary</h2>
<ul>
<li><strong>Probability Mass Function (PMF)</strong>: a
representation of a distribution as a function that maps from values
to probabilities.</li>
<li><strong>probability</strong>: A frequency expressed as
a fraction of the sample size.</li>
<li><strong>normalization</strong>: The process of
dividing a frequency by a sample size to get a probability.</li>
<li><strong>index</strong>: In a pandas DataFrame, the
index is a special column that contains the row labels.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distributions.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
