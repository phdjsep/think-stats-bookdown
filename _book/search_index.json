[
["index.html", "Think Stats Exploratory Data Analysis in Python Preface 0.1 How I wrote this book 0.2 Using the code 0.3 Contributor List", " Think Stats Exploratory Data Analysis in Python Allen B. Downey 2019-03-23 Preface You might prefer to read the PDF version, or you can buy a hardcopy from Amazon. This book is an introduction to the practical tools of exploratory data analysis. The organization of the book follows the process I use when I start working with a dataset: Importing and cleaning: Whatever format the data is in, it usually takes some time and effort to read the data, clean and transform it, and check that everything made it through the translation process intact. Single variable explorations: I usually start by examining one variable at a time, finding out what the variables mean, looking at distributions of the values, and choosing appropriate summary statistics. Pair-wise explorations: To identify possible relationships between variables, I look at tables and scatter plots, and compute correlations and linear fits. Multivariate analysis: If there are apparent relationships between variables, I use multiple regression to add control variables and investigate more complex relationships. Estimation and hypothesis testing: When reporting statistical results, it is important to answer three questions: How big is the effect? How much variability should we expect if we run the same measurement again? Is it possible that the apparent effect is due to chance? Visualization: During exploration, visualization is an important tool for finding possible relationships and effects. Then if an apparent effect holds up to scrutiny, visualization is an effective way to communicate results. This book takes a computational approach, which has several advantages over mathematical approaches: I present most ideas using Python code, rather than mathematical notation. In general, Python code is more readable; also, because it is executable, readers can download it, run it, and modify it. Each chapter includes exercises readers can do to develop and solidify their learning. When you write programs, you express your understanding in code; while you are debugging the program, you are also correcting your understanding. Some exercises involve experiments to test statistical behavior. For example, you can explore the Central Limit Theorem (CLT) by generating random samples and computing their sums. The resulting visualizations demonstrate why the CLT works and when it doesn’t. Some ideas that are hard to grasp mathematically are easy to understand by simulation. For example, we approximate p-values by running random simulations, which reinforces the meaning of the p-value. Because the book is based on a general-purpose programming language (Python), readers can import data from almost any source. They are not limited to datasets that have been cleaned and formatted for a particular statistics tool. The book lends itself to a project-based approach. In my class, students work on a semester-long project that requires them to pose a statistical question, find a dataset that can address it, and apply each of the techniques they learn to their own data. To demonstrate my approach to statistical analysis, the book presents a case study that runs through all of the chapters. It uses data from two sources: The National Survey of Family Growth (NSFG), conducted by the U.S. Centers for Disease Control and Prevention (CDC) to gather “information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health.” (See http://cdc.gov/nchs/nsfg.htm.) The Behavioral Risk Factor Surveillance System (BRFSS), conducted by the National Center for Chronic Disease Prevention and Health Promotion to “track health conditions and risk behaviors in the United States.” (See http://cdc.gov/BRFSS/.) Other examples use data from the IRS, the U.S. Census, and the Boston Marathon. This second edition of Think Stats includes the chapters from the first edition, many of them substantially revised, and new chapters on regression, time series analysis, survival analysis, and analytic methods. The previous edition did not use pandas, SciPy, or StatsModels, so all of that material is new. 0.1 How I wrote this book When people write a new textbook, they usually start by reading a stack of old textbooks. As a result, most books contain the same material in pretty much the same order. I did not do that. In fact, I used almost no printed material while I was writing this book, for several reasons: My goal was to explore a new approach to this material, so I didn’t want much exposure to existing approaches. Since I am making this book available under a free license, I wanted to make sure that no part of it was encumbered by copyright restrictions. Many readers of my books don’t have access to libraries of printed material, so I tried to make references to resources that are freely available on the Internet. Some proponents of old media think that the exclusive use of electronic resources is lazy and unreliable. They might be right about the first part, but I think they are wrong about the second, so I wanted to test my theory. The resource I used more than any other is Wikipedia. In general, the articles I read on statistical topics were very good (although I made a few small changes along the way). I include references to Wikipedia pages throughout the book and I encourage you to follow those links; in many cases, the Wikipedia page picks up where my description leaves off. The vocabulary and notation in this book are generally consistent with Wikipedia, unless I had a good reason to deviate. Other resources I found useful were Wolfram MathWorld and the Reddit statistics forum, http://www.reddit.com/r/statistics. 0.2 Using the code The code and data used in this book are available from https://github.com/AllenDowney/ThinkStats2. Git is a version control system that allows you to keep track of the files that make up a project. A collection of files under Git’s control is called a repository. GitHub is a hosting service that provides storage for Git repositories and a convenient web interface. The GitHub homepage for my repository provides several ways to work with the code: You can create a copy of my repository on GitHub by pressing the Fork button. If you don’t already have a GitHub account, you’ll need to create one. After forking, you’ll have your own repository on GitHub that you can use to keep track of code you write while working on this book. Then you can clone the repo, which means that you make a copy of the files on your computer. Or you could clone my repository. You don’t need a GitHub account to do this, but you won’t be able to write your changes back to GitHub. If you don’t want to use Git at all, you can download the files in a Zip file using the button in the lower-right corner of the GitHub page. All of the code is written to work in both Python 2 and Python 3 with no translation. I developed this book using Anaconda from Continuum Analytics, which is a free Python distribution that includes all the packages you’ll need to run the code (and lots more). I found Anaconda easy to install. By default it does a user-level installation, not system-level, so you don’t need administrative privileges. And it supports both Python 2 and Python 3. You can download Anaconda from http://continuum.io/downloads. If you don’t want to use Anaconda, you will need the following packages: pandas for representing and analyzing data, http://pandas.pydata.org/; NumPy for basic numerical computation, http://www.numpy.org/; SciPy for scientific computation including statistics, http://www.scipy.org/; StatsModels for regression and other statistical analysis, http://statsmodels.sourceforge.net/; and matplotlib for visualization, http://matplotlib.org/. Although these are commonly used packages, they are not included with all Python installations, and they can be hard to install in some environments. If you have trouble installing them, I strongly recommend using Anaconda or one of the other Python distributions that include these packages. After you clone the repository or unzip the zip file, you should have a folder called ThinkStats2/code with a file called nsfg.py. If you run nsfg.py, it should read a data file, run some tests, and print a message like, “All tests passed.” If you get import errors, it probably means there are packages you need to install. Most exercises use Python scripts, but some also use the IPython notebook. If you have not used IPython notebook before, I suggest you start with the documentation at http://ipython.org/ipython-doc/stable/notebook/notebook.html. I wrote this book assuming that the reader is familiar with core Python, including object-oriented features, but not pandas, NumPy, and SciPy. If you are already familiar with these modules, you can skip a few sections. I assume that the reader knows basic mathematics, including logarithms, for example, and summations. I refer to calculus concepts in a few places, but you don’t have to do any calculus. If you have never studied statistics, I think this book is a good place to start. And if you have taken a traditional statistics class, I hope this book will help repair the damage. — Allen B. Downey is a Professor of Computer Science at the Franklin W. Olin College of Engineering in Needham, MA. 0.3 Contributor List If you have a suggestion or correction, please send email to downey@allendowney.com. If I make a change based on your feedback, I will add you to the contributor list (unless you ask to be omitted). If you include at least part of the sentence the error appears in, that makes it easy for me to search. Page and section numbers are fine, too, but not quite as easy to work with. Thanks! Lisa Downey and June Downey read an early draft and made many corrections and suggestions. Steven Zhang found several errors. Andy Pethan and Molly Farison helped debug some of the solutions, and Molly spotted several typos. Dr. Nikolas Akerblom knows how big a Hyracotherium is. Alex Morrow clarified one of the code examples. Jonathan Street caught an error in the nick of time. Many thanks to Kevin Smith and Tim Arnold for their work on plasTeX, which I used to convert this book to DocBook. George Caplan sent several suggestions for improving clarity. Julian Ceipek found an error and a number of typos. Stijn Debrouwere, Leo Marihart III, Jonathan Hammler, and Kent Johnson found errors in the first print edition. Jörg Beyer found typos in the book and made many corrections in the docstrings of the accompanying code. Tommie Gannert sent a patch file with a number of corrections. Christoph Lendenmann submitted several errata. Michael Kearney sent me many excellent suggestions. Alex Birch made a number of helpful suggestions. Lindsey Vanderlyn, Griffin Tschurwald, and Ben Small read an early version of this book and found many errors. John Roth, Carol Willing, and Carol Novitsky performed technical reviews of the book. They found many errors and made many helpful suggestions. David Palmer sent many helpful suggestions and corrections. Erik Kulyk found many typos. Nir Soffer sent several excellent pull requests for both the book and the supporting code. GitHub user flothesof sent a number of corrections. Toshiaki Kurokawa, who is working on the Japanese translation of this book, has sent many corrections and helpful suggestions. Benjamin White suggested more idiomatic Pandas code. Takashi Sato spotted an code error. Other people who found typos and similar errors are Andrew Heine, Gábor Lipták, Dan Kearney, Alexander Gryzlov, Martin Veillette, Haitao Ma, Jeff Pickhardt, Rohit Deshpande, Joanne Pratt, Lucian Ursu, Paul Glezen, Ting-kuang Lin. "],
["exploratory.html", "Chapter 1 Exploratory Data Analysis 1.1 A Statistical Approach 1.2 The National Survey of Family Growth 1.3 Importing the Data 1.4 DataFrames 1.5 Variables 1.6 Transformation 1.7 Validation 1.8 Interpretation 1.9 Exercises 1.10 Glossary", " Chapter 1 Exploratory Data Analysis The thesis of this book is that data combined with practical methods can answer questions and guide decisions under uncertainty. As an example, I present a case study motivated by a question I heard when my wife and I were expecting our first child: do first babies tend to arrive late? If you Google this question, you will find plenty of discussion. Some people claim it’s true, others say it’s a myth, and some people say it’s the other way around: first babies come early. In many of these discussions, people provide data to support their claims. I found many examples like these: “My two friends that have given birth recently to their first babies, BOTH went almost 2 weeks overdue before going into labour or being induced.” “My first one came 2 weeks late and now I think the second one is going to come out two weeks early!!” “I don’t think that can be true because my sister was my mother’s first and she was early, as with many of my cousins.” Reports like these are called anecdotal evidence because they are based on data that is unpublished and usually personal. In casual conversation, there is nothing wrong with anecdotes, so I don’t mean to pick on the people I quoted. But we might want evidence that is more persuasive and an answer that is more reliable. By those standards, anecdotal evidence usually fails, because: Small number of observations: If pregnancy length is longer for first babies, the difference is probably small compared to natural variation. In that case, we might have to compare a large number of pregnancies to be sure that a difference exists. Selection bias: People who join a discussion of this question might be interested because their first babies were late. In that case the process of selecting data would bias the results. Confirmation bias: People who believe the claim might be more likely to contribute examples that confirm it. People who doubt the claim are more likely to cite counterexamples. Inaccuracy: Anecdotes are often personal stories, and often misremembered, misrepresented, repeated inaccurately, etc. So how can we do better? 1.1 A Statistical Approach To address the limitations of anecdotes, we will use the tools of statistics, which include: Data collection: We will use data from a large national survey that was designed explicitly with the goal of generating statistically valid inferences about the U.S. population. Descriptive statistics: We will generate statistics that summarize the data concisely, and evaluate different ways to visualize data. Exploratory data analysis: We will look for patterns, differences, and other features that address the questions we are interested in. At the same time we will check for inconsistencies and identify limitations. Estimation: We will use data from a sample to estimate characteristics of the general population. Hypothesis testing: Where we see apparent effects, like a difference between two groups, we will evaluate whether the effect might have happened by chance. By performing these steps with care to avoid pitfalls, we can reach conclusions that are more justifiable and more likely to be correct. 1.2 The National Survey of Family Growth Since 1973 the U.S. Centers for Disease Control and Prevention (CDC) have conducted the National Survey of Family Growth (NSFG), which is intended to gather “information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health. The survey results are used … to plan health services and health education programs, and to do statistical studies of families, fertility, and health.” See http://cdc.gov/nchs/nsfg.htm. We will use data collected by this survey to investigate whether first babies tend to come late, and other questions. In order to use this data effectively, we have to understand the design of the study. The NSFG is a cross-sectional study, which means that it captures a snapshot of a group at a point in time. The most common alternative is a longitudinal study, which observes a group repeatedly over a period of time. The NSFG has been conducted seven times; each deployment is called a cycle. We will use data from Cycle 6, which was conducted from January 2002 to March 2003. The goal of the survey is to draw conclusions about a population; the target population of the NSFG is people in the United States aged 15-44. Ideally surveys would collect data from every member of the population, but that’s seldom possible. Instead we collect data from a subset of the population called a sample. The people who participate in a survey are called respondents. In general, cross-sectional studies are meant to be representative, which means that every member of the target population has an equal chance of participating. That ideal is hard to achieve in practice, but people who conduct surveys come as close as they can. The NSFG is not representative; instead it is deliberately oversampled. The designers of the study recruited three groups—Hispanics, African-Americans and teenagers—at rates higher than their representation in the U.S. population, in order to make sure that the number of respondents in each of these groups is large enough to draw valid statistical inferences. Of course, the drawback of oversampling is that it is not as easy to draw conclusions about the general population based on statistics from the survey. We will come back to this point later. When working with this kind of data, it is important to be familiar with the codebook, which documents the design of the study, the survey questions, and the encoding of the responses. The codebook and user’s guide for the NSFG data are available from http://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm 1.3 Importing the Data The code and data used in this book are available from https://github.com/AllenDowney/ThinkStats2. For information about downloading and working with this code, see Section 0.2. Once you download the code, you should have a file called ThinkStats2/code/nsfg.py. If you run it, it should read a data file, run some tests, and print a message like, “All tests passed.” Let’s see what it does. Pregnancy data from Cycle 6 of the NSFG is in a file called 2002FemPreg.dat.gz; it is a gzip-compressed data file in plain text (ASCII), with fixed width columns. Each line in the file is a record that contains data about one pregnancy. The format of the file is documented in 2002FemPreg.dct, which is a Stata dictionary file. Stata is a statistical software system; a “dictionary” in this context is a list of variable names, types, and indices that identify where in each line to find each variable. For example, here are a few lines from 2002FemPreg.dct: infile dictionary { _column(1) str12 caseid %12s &quot;RESPONDENT ID NUMBER&quot; _column(13) byte pregordr %2f &quot;PREGNANCY ORDER (NUMBER)&quot; } This dictionary describes two variables: caseid is a 12-character string that represents the respondent ID; pregorder is a one-byte integer that indicates which pregnancy this record describes for this respondent. The code you downloaded includes thinkstats2.py, which is a Python module that contains many classes and functions used in this book, including functions that read the Stata dictionary and the NSFG data file. Here’s how they are used in nsfg.py: def ReadFemPreg(dct_file=&#39;2002FemPreg.dct&#39;, dat_file=&#39;2002FemPreg.dat.gz&#39;): dct = thinkstats2.ReadStataDct(dct_file) df = dct.ReadFixedWidth(dat_file, compression=&#39;gzip&#39;) CleanFemPreg(df) return df ReadStataDct takes the name of the dictionary file and returns dct, a FixedWidthVariables object that contains the information from the dictionary file. dct provides ReadFixedWidth, which reads the data file. 1.4 DataFrames The result of ReadFixedWidth is a DataFrame, which is the fundamental data structure provided by pandas, which is a Python data and statistics package we’ll use throughout this book. A DataFrame contains a row for each record, in this case one row per pregnancy, and a column for each variable. In addition to the data, a DataFrame also contains the variable names and their types, and it provides methods for accessing and modifying the data. If you print df you get a truncated view of the rows and columns, and the shape of the DataFrame, which is 13593 rows/records and 244 columns/variables. &gt;&gt;&gt; import nsfg &gt;&gt;&gt; df = nsfg.ReadFemPreg() &gt;&gt;&gt; df ... [13593 rows x 244 columns] The DataFrame is too big to display, so the output is truncated. The last line reports the number of rows and columns. The attribute columns returns a sequence of column names as Unicode strings: &gt;&gt;&gt; df.columns Index([u&#39;caseid&#39;, u&#39;pregordr&#39;, u&#39;howpreg_n&#39;, u&#39;howpreg_p&#39;, ... ]) The result is an Index, which is another pandas data structure. We’ll learn more about Index later, but for now we’ll treat it like a list: &gt;&gt;&gt; df.columns[1] &#39;pregordr&#39; To access a column from a DataFrame, you can use the column name as a key: &gt;&gt;&gt; pregordr = df[&#39;pregordr&#39;] &gt;&gt;&gt; type(pregordr) &lt;class &#39;pandas.core.series.Series&#39;&gt; The result is a Series, yet another pandas data structure. A Series is like a Python list with some additional features. When you print a Series, you get the indices and the corresponding values: &gt;&gt;&gt; pregordr 0 1 1 2 2 1 3 2 ... 13590 3 13591 4 13592 5 Name: pregordr, Length: 13593, dtype: int64 In this example the indices are integers from 0 to 13592, but in general they can be any sortable type. The elements are also integers, but they can be any type. The last line includes the variable name, Series length, and data type; int64 is one of the types provided by NumPy. If you run this example on a 32-bit machine you might see int32. You can access the elements of a Series using integer indices and slices: &gt;&gt;&gt; pregordr[0] 1 &gt;&gt;&gt; pregordr[2:5] 2 1 3 2 4 3 Name: pregordr, dtype: int64 The result of the index operator is an int64; the result of the slice is another Series. You can also access the columns of a DataFrame using dot notation: &gt;&gt;&gt; pregordr = df.pregordr This notation only works if the column name is a valid Python identifier, so it has to begin with a letter, can’t contain spaces, etc. 1.5 Variables We have already seen two variables in the NSFG dataset, caseid and pregordr, and we have seen that there are 244 variables in total. For the explorations in this book, I use the following variables: caseid is the integer ID of the respondent. prglngth is the integer duration of the pregnancy in weeks. outcome is an integer code for the outcome of the pregnancy. The code 1 indicates a live birth. pregordr is a pregnancy serial number; for example, the code for a respondent’s first pregnancy is 1, for the second pregnancy is 2, and so on. birthord is a serial number for live births; the code for a respondent’s first child is 1, and so on. For outcomes other than live birth, this field is blank. birthwgt_lb and birthwgt_oz contain the pounds and ounces parts of the birth weight of the baby. agepreg is the mother’s age at the end of the pregnancy. finalwgt is the statistical weight associated with the respondent. It is a floating-point value that indicates the number of people in the U.S. population this respondent represents. If you read the codebook carefully, you will see that many of the variables are recodes, which means that they are not part of the raw data collected by the survey; they are calculated using the raw data. For example, prglngth for live births is equal to the raw variable wksgest (weeks of gestation) if it is available; otherwise it is estimated using mosgest * 4.33 (months of gestation times the average number of weeks in a month). Recodes are often based on logic that checks the consistency and accuracy of the data. In general it is a good idea to use recodes when they are available, unless there is a compelling reason to process the raw data yourself. 1.6 Transformation When you import data like this, you often have to check for errors, deal with special values, convert data into different formats, and perform calculations. These operations are called data cleaning. nsfg.py includes CleanFemPreg, a function that cleans the variables I am planning to use. def CleanFemPreg(df): df.agepreg /= 100.0 na_vals = [97, 98, 99] df.birthwgt_lb.replace(na_vals, np.nan, inplace=True) df.birthwgt_oz.replace(na_vals, np.nan, inplace=True) df[&#39;totalwgt_lb&#39;] = df.birthwgt_lb + df.birthwgt_oz / 16.0 agepreg contains the mother’s age at the end of the pregnancy. In the data file, agepreg is encoded as an integer number of centiyears. So the first line divides each element of agepreg by 100, yielding a floating-point value in years. birthwgt_lb and birthwgt_oz contain the weight of the baby, in pounds and ounces, for pregnancies that end in live birth. In addition it uses several special codes: 97 NOT ASCERTAINED 98 REFUSED 99 DON&#39;T KNOW Special values encoded as numbers are dangerous because if they are not handled properly, they can generate bogus results, like a 99-pound baby. The replace method replaces these values with np.nan, a special floating-point value that represents “not a number.” The inplace flag tells replace to modify the existing Series rather than create a new one. As part of the IEEE floating-point standard, all mathematical operations return nan if either argument is nan: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; np.nan / 100.0 nan So computations with nan tend to do the right thing, and most pandas functions handle nan appropriately. But dealing with missing data will be a recurring issue. The last line of CleanFemPreg creates a new column totalwgt_lb that combines pounds and ounces into a single quantity, in pounds. One important note: when you add a new column to a DataFrame, you must use dictionary syntax, like this # CORRECT df[&#39;totalwgt_lb&#39;] = df.birthwgt_lb + df.birthwgt_oz / 16.0 Not dot notation, like this: # WRONG! df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0 The version with dot notation adds an attribute to the DataFrame object, but that attribute is not treated as a new column. 1.7 Validation When data is exported from one software environment and imported into another, errors might be introduced. And when you are getting familiar with a new dataset, you might interpret data incorrectly or introduce other misunderstandings. If you take time to validate the data, you can save time later and avoid errors. One way to validate data is to compute basic statistics and compare them with published results. For example, the NSFG codebook includes tables that summarize each variable. Here is the table for outcome, which encodes the outcome of each pregnancy: value label Total 1 LIVE BIRTH 9148 2 INDUCED ABORTION 1862 3 STILLBIRTH 120 4 MISCARRIAGE 1921 5 ECTOPIC PREGNANCY 190 6 CURRENT PREGNANCY 352 The Series class provides a method, value_counts, that counts the number of times each value appears. If we select the outcome Series from the DataFrame, we can use value_counts to compare with the published data: &gt;&gt;&gt; df.outcome.value_counts(sort=False) 1 9148 2 1862 3 120 4 1921 5 190 6 352 The result of value_counts is a Series; sort=False doesn’t sort the Series by values, so them appear in order. Comparing the results with the published table, it looks like the values in outcome are correct. Similarly, here is the published table for birthwgt_lb value label Total . INAPPLICABLE 4449 0-5 UNDER 6 POUNDS 1125 6 6 POUNDS 2223 7 7 POUNDS 3049 8 8 POUNDS 1889 9-95 9 POUNDS OR MORE 799 And here are the value counts: &gt;&gt;&gt; df.birthwgt_lb.value_counts(sort=False) 0 8 1 40 2 53 3 98 4 229 5 697 6 2223 7 3049 8 1889 9 623 10 132 11 26 12 10 13 3 14 3 15 1 51 1 The counts for 6, 7, and 8 pounds check out, and if you add up the counts for 0-5 and 9-95, they check out, too. But if you look more closely, you will notice one value that has to be an error, a 51 pound baby! To deal with this error, I added a line to CleanFemPreg: df.loc[df.birthwgt_lb &gt; 20, &#39;birthwgt_lb&#39;] = np.nan This statement replaces invalid values with np.nan. The attribute loc provides several ways to select rows and columns from a DataFrame. In this example, the first expression in brackets is the row indexer; the second expression selects the column. The expression df.birthwgt_lb &gt; 20 yields a Series of type bool, where True indicates that the condition is true. When a boolean Series is used as an index, it selects only the elements that satisfy the condition. 1.8 Interpretation To work with data effectively, you have to think on two levels at the same time: the level of statistics and the level of context. As an example, let’s look at the sequence of outcomes for a few respondents. Because of the way the data files are organized, we have to do some processing to collect the pregnancy data for each respondent. Here’s a function that does that: def MakePregMap(df): d = defaultdict(list) for index, caseid in df.caseid.iteritems(): d[caseid].append(index) return d df is the DataFrame with pregnancy data. The iteritems method enumerates the index (row number) and caseid for each pregnancy. d is a dictionary that maps from each case ID to a list of indices. If you are not familiar with defaultdict, it is in the Python collections module. Using d, we can look up a respondent and get the indices of that respondent’s pregnancies. This example looks up one respondent and prints a list of outcomes for her pregnancies: &gt;&gt;&gt; caseid = 10229 &gt;&gt;&gt; preg_map = nsfg.MakePregMap(df) &gt;&gt;&gt; indices = preg_map[caseid] &gt;&gt;&gt; df.outcome[indices].values [4 4 4 4 4 4 1] indices is the list of indices for pregnancies corresponding to respondent 10229. Using this list as an index into df.outcome selects the indicated rows and yields a Series. Instead of printing the whole Series, I selected the values attribute, which is a NumPy array. The outcome code 1 indicates a live birth. Code 4 indicates a miscarriage; that is, a pregnancy that ended spontaneously, usually with no known medical cause. Statistically this respondent is not unusual. Miscarriages are common and there are other respondents who reported as many or more. But remembering the context, this data tells the story of a woman who was pregnant six times, each time ending in miscarriage. Her seventh and most recent pregnancy ended in a live birth. If we consider this data with empathy, it is natural to be moved by the story it tells. Each record in the NSFG dataset represents a person who provided honest answers to many personal and difficult questions. We can use this data to answer statistical questions about family life, reproduction, and health. At the same time, we have an obligation to consider the people represented by the data, and to afford them respect and gratitude. 1.9 Exercises Exercise 1 In the repository you downloaded, you should find a file named chap01ex.ipynb, which is an IPython notebook. You can launch IPython notebook from the command line like this: $ ipython notebook &amp; If IPython is installed, it should launch a server that runs in the background and open a browser to view the notebook. If you are not familiar with IPython, I suggest you start at http://ipython.org/ipython-doc/stable/notebook/notebook.html. To launch the IPython notebook server, run: $ ipython notebook &amp; It should open a new browser window, but if not, the startup message provides a URL you can load in a browser, usually http://localhost:8888. The new window should list the notebooks in the repository. Open chap01ex.ipynb. Some cells are already filled in, and you should execute them. Other cells give you instructions for exercises you should try. A solution to this exercise is in chap01soln.ipynb Exercise 2 In the repository you downloaded, you should find a file named chap01ex.py; using this file as a starting place, write a function that reads the respondent file, 2002FemResp.dat.gz. The variable pregnum is a recode that indicates how many times each respondent has been pregnant. Print the value counts for this variable and compare them to the published results in the NSFG codebook. You can also cross-validate the respondent and pregnancy files by comparing pregnum for each respondent with the number of records in the pregnancy file. You can use nsfg.MakePregMap to make a dictionary that maps from each caseid to a list of indices into the pregnancy DataFrame. A solution to this exercise is in chap01soln.py Exercise 3 The best way to learn about statistics is to work on a project you are interested in. Is there a question like, “Do first babies arrive late,” that you want to investigate? Think about questions you find personally interesting, or items of conventional wisdom, or controversial topics, or questions that have political consequences, and see if you can formulate a question that lends itself to statistical inquiry. Look for data to help you address the question. Governments are good sources because data from public research is often freely available. Good places to start include http://www.data.gov/, and http://www.science.gov/, and in the United Kingdom, http://data.gov.uk/. Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, and the European Social Survey at http://www.europeansocialsurvey.org/. If it seems like someone has already answered your question, look closely to see whether the answer is justified. There might be flaws in the data or the analysis that make the conclusion unreliable. In that case you could perform a different analysis of the same data, or look for a better source of data. If you find a published paper that addresses your question, you should be able to get the raw data. Many authors make their data available on the web, but for sensitive data you might have to write to the authors, provide information about how you plan to use the data, or agree to certain terms of use. Be persistent! 1.10 Glossary anecdotal evidence: Evidence, often personal, that is collected casually rather than by a well-designed study. population: A group we are interested in studying. “Population” often refers to a group of people, but the term is used for other subjects, too. cross-sectional study: A study that collects data about a population at a particular point in time. cycle: In a repeated cross-sectional study, each repetition of the study is called a cycle. longitudinal study: A study that follows a population over time, collecting data from the same group repeatedly. record: In a dataset, a collection of information about a single person or other subject. respondent: A person who responds to a survey. sample: The subset of a population used to collect data. representative: A sample is representative if every member of the population has the same chance of being in the sample. oversampling: The technique of increasing the representation of a sub-population in order to avoid errors due to small sample sizes. raw data: Values collected and recorded with little or no checking, calculation or interpretation. recode: A value that is generated by calculation and other logic applied to raw data. data cleaning: Processes that include validating data, identifying errors, translating between data types and representations, etc. "],
["distributions.html", "Chapter 2 Distributions 2.1 Histograms 2.2 Representing Histograms 2.3 Plotting Histograms 2.4 NSFG Variables 2.5 Outliers 2.6 First Babies 2.7 Summarizing Distributions 2.8 Variance 2.9 Effect Size 2.10 Reporting Results 2.11 Exercises 2.12 Glossary", " Chapter 2 Distributions 2.1 Histograms One of the best ways to describe a variable is to report the values that appear in the dataset and how many times each value appears. This description is called the distribution of the variable. The most common representation of a distribution is a histogram, which is a graph that shows the frequency of each value. In this context, “frequency” means the number of times the value appears. In Python, an efficient way to compute frequencies is with a dictionary. Given a sequence of values, t: hist = {} for x in t: hist[x] = hist.get(x, 0) + 1 The result is a dictionary that maps from values to frequencies. Alternatively, you could use the Counter class defined in the collections module: from collections import Counter counter = Counter(t) The result is a Counter object, which is a subclass of dictionary. Another option is to use the pandas method value_counts, which we saw in the previous chapter. But for this book I created a class, Hist, that represents histograms and provides the methods that operate on them. 2.2 Representing Histograms The Hist constructor can take a sequence, dictionary, pandas Series, or another Hist. You can instantiate a Hist object like this: &gt;&gt;&gt; import thinkstats2 &gt;&gt;&gt; hist = thinkstats2.Hist([1, 2, 2, 3, 5]) &gt;&gt;&gt; hist Hist({1: 1, 2: 2, 3: 1, 5: 1}) Hist objects provide Freq, which takes a value and returns its frequency: &gt;&gt;&gt; hist.Freq(2) 2 The bracket operator does the same thing: &gt;&gt;&gt; hist[2] 2 If you look up a value that has never appeared, the frequency is 0. &gt;&gt;&gt; hist.Freq(4) 0 Values returns an unsorted list of the values in the Hist: &gt;&gt;&gt; hist.Values() [1, 5, 3, 2] To loop through the values in order, you can use the built-in function sorted: for val in sorted(hist.Values()): print(val, hist.Freq(val)) Or you can use Items to iterate through value-frequency pairs: for val, freq in hist.Items(): print(val, freq) 2.3 Plotting Histograms Figure 2.1: Histogram of the pound part of birth weight. For this book I wrote a module called thinkplot.py that provides functions for plotting Hists and other objects defined in thinkstats2.py. It is based on pyplot, which is part of the matplotlib package. See Section 0.2 for information about installing matplotlib. To plot hist with thinkplot, try this: &gt;&gt;&gt; import thinkplot &gt;&gt;&gt; thinkplot.Hist(hist) &gt;&gt;&gt; thinkplot.Show(xlabel=&#39;value&#39;, ylabel=&#39;frequency&#39;) You can read the documentation for thinkplot at http://greenteapress.com/thinkstats2/thinkplot.html Figure 2.2: Histogram of the ounce part of birth weight. 2.4 NSFG Variables Now let’s get back to the data from the NSFG. The code in this chapter is in first.py. For information about downloading and working with this code, see Section 0.2. When you start working with a new dataset, I suggest you explore the variables you are planning to use one at a time, and a good way to start is by looking at histograms. In Section 1.6 we transformed agepreg from centiyears to years, and combined birthwgt_lb and birthwgt_oz into a single quantity, totalwgt_lb. In this section I use these variables to demonstrate some features of histograms. Figure 2.3: Histogram of mother’s age at end of pregnancy. I’ll start by reading the data and selecting records for live births: preg = nsfg.ReadFemPreg() live = preg[preg.outcome == 1] The expression in brackets is a boolean Series that selects rows from the DataFrame and returns a new DataFrame. Next I generate and plot the histogram of birthwgt_lb for live births. hist = thinkstats2.Hist(live.birthwgt_lb, label=&#39;birthwgt_lb&#39;) thinkplot.Hist(hist) thinkplot.Show(xlabel=&#39;pounds&#39;, ylabel=&#39;frequency&#39;) When the argument passed to Hist is a pandas Series, any nan values are dropped. label is a string that appears in the legend when the Hist is plotted. Figure 2.4: Histogram of pregnancy length in weeks. Figure 2.1 shows the result. The most common value, called the mode, is 7 pounds. The distribution is approximately bell-shaped, which is the shape of the normal distribution, also called a Gaussian distribution. But unlike a true normal distribution, this distribution is asymmetric; it has a tail that extends farther to the left than to the right. Figure 2.2 shows the histogram of birthwgt_oz, which is the ounces part of birth weight. In theory we expect this distribution to be uniform; that is, all values should have the same frequency. In fact, 0 is more common than the other values, and 1 and 15 are less common, probably because respondents round off birth weights that are close to an integer value. Figure 2.3 shows the histogram of agepreg, the mother’s age at the end of pregnancy. The mode is 21 years. The distribution is very roughly bell-shaped, but in this case the tail extends farther to the right than left; most mothers are in their 20s, fewer in their 30s. Figure 2.4 shows the histogram of prglngth, the length of the pregnancy in weeks. By far the most common value is 39 weeks. The left tail is longer than the right; early babies are common, but pregnancies seldom go past 43 weeks, and doctors often intervene if they do. 2.5 Outliers Looking at histograms, it is easy to identify the most common values and the shape of the distribution, but rare values are not always visible. Before going on, it is a good idea to check for outliers, which are extreme values that might be errors in measurement and recording, or might be accurate reports of rare events. Hist provides methods Largest and Smallest, which take an integer n and return the n largest or smallest values from the histogram: for weeks, freq in hist.Smallest(10): print(weeks, freq) In the list of pregnancy lengths for live births, the 10 lowest values are [0, 4, 9, 13, 17, 18, 19, 20, 21, 22\\]. Values below 10 weeks are certainly errors; the most likely explanation is that the outcome was not coded correctly. Values higher than 30 weeks are probably legitimate. Between 10 and 30 weeks, it is hard to be sure; some values are probably errors, but some represent premature babies. On the other end of the range, the highest values are: weeks count 43 148 44 46 45 10 46 1 47 1 48 7 50 2 Most doctors recommend induced labor if a pregnancy exceeds 42 weeks, so some of the longer values are surprising. In particular, 50 weeks seems medically unlikely. The best way to handle outliers depends on “domain knowledge”; that is, information about where the data come from and what they mean. And it depends on what analysis you are planning to perform. In this example, the motivating question is whether first babies tend to be early (or late). When people ask this question, they are usually interested in full-term pregnancies, so for this analysis I will focus on pregnancies longer than 27 weeks. 2.6 First Babies Now we can compare the distribution of pregnancy lengths for first babies and others. I divided the DataFrame of live births using birthord, and computed their histograms: firsts = live[live.birthord == 1] others = live[live.birthord != 1] first_hist = thinkstats2.Hist(firsts.prglngth) other_hist = thinkstats2.Hist(others.prglngth) Then I plotted their histograms on the same axis: width = 0.45 thinkplot.PrePlot(2) thinkplot.Hist(first_hist, align=&#39;right&#39;, width=width) thinkplot.Hist(other_hist, align=&#39;left&#39;, width=width) thinkplot.Show(xlabel=&#39;weeks&#39;, ylabel=&#39;frequency&#39;, xlim=[27, 46]) thinkplot.PrePlot takes the number of histograms we are planning to plot; it uses this information to choose an appropriate collection of colors. Figure 2.5: Histogram of pregnancy lengths. thinkplot.Hist normally uses align=’center’ so that each bar is centered over its value. For this figure, I use align=’right’ and align=’left’ to place corresponding bars on either side of the value. With width=0.45, the total width of the two bars is 0.9, leaving some space between each pair. Finally, I adjust the axis to show only data between 27 and 46 weeks. Figure 2.5 shows the result. Histograms are useful because they make the most frequent values immediately apparent. But they are not the best choice for comparing two distributions. In this example, there are fewer “first babies” than “others,” so some of the apparent differences in the histograms are due to sample sizes. In the next chapter we address this problem using probability mass functions. 2.7 Summarizing Distributions A histogram is a complete description of the distribution of a sample; that is, given a histogram, we could reconstruct the values in the sample (although not their order). If the details of the distribution are important, it might be necessary to present a histogram. But often we want to summarize the distribution with a few descriptive statistics. Some of the characteristics we might want to report are: central tendency: Do the values tend to cluster around a particular point? modes: Is there more than one cluster? spread: How much variability is there in the values? tails: How quickly do the probabilities drop off as we move away from the modes? outliers: Are there extreme values far from the modes? Statistics designed to answer these questions are called summary statistics. By far the most common summary statistic is the mean, which is meant to describe the central tendency of the distribution. If you have a sample of n values, \\(x_i\\), the mean, x, is the sum of the values divided by the number of values; in other words \\[ \\bar{x} = \\frac 1n \\sum_i x_i \\] The words “mean” and “average” are sometimes used interchangeably, but I make this distinction: The “mean” of a sample is the summary statistic computed with the previous formula. An “average” is one of several summary statistics you might choose to describe a central tendency. Sometimes the mean is a good description of a set of values. For example, apples are all pretty much the same size (at least the ones sold in supermarkets). So if I buy 6 apples and the total weight is 3 pounds, it would be a reasonable summary to say they are about a half pound each. But pumpkins are more diverse. Suppose I grow several varieties in my garden, and one day I harvest three decorative pumpkins that are 1 pound each, two pie pumpkins that are 3 pounds each, and one Atlantic Giant® pumpkin that weighs 591 pounds. The mean of this sample is 100 pounds, but if I told you “The average pumpkin in my garden is 100 pounds,” that would be misleading. In this example, there is no meaningful average because there is no typical pumpkin. 2.8 Variance If there is no single number that summarizes pumpkin weights, we can do a little better with two numbers: mean and variance. Variance is a summary statistic intended to describe the variability or spread of a distribution. The variance of a set of values is \\[ S^2 = \\frac 1n \\sum_i (x_i - \\bar x)^2 \\] The term \\(x_i\\) − x is called the “deviation from the mean,” so variance is the mean squared deviation. The square root of variance, \\(S\\), is the standard deviation. If you have prior experience, you might have seen a formula for variance with \\(n-1\\) in the denominator, rather than n. This statistic is used to estimate the variance in a population using a sample. We will come back to this in Chapter 8. Pandas data structures provides methods to compute mean, variance and standard deviation: mean = live.prglngth.mean() var = live.prglngth.var() std = live.prglngth.std() For all live births, the mean pregnancy length is 38.6 weeks, the standard deviation is 2.7 weeks, which means we should expect deviations of 2-3 weeks to be common. Variance of pregnancy length is 7.3, which is hard to interpret, especially since the units are weeks2, or “square weeks.” Variance is useful in some calculations, but it is not a good summary statistic. 2.9 Effect Size An effect size is a summary statistic intended to describe (wait for it) the size of an effect. For example, to describe the difference between two groups, one obvious choice is the difference in the means. Mean pregnancy length for first babies is 38.601; for other babies it is 38.523. The difference is 0.078 weeks, which works out to 13 hours. As a fraction of the typical pregnancy length, this difference is about 0.2%. If we assume this estimate is accurate, such a difference would have no practical consequences. In fact, without observing a large number of pregnancies, it is unlikely that anyone would notice this difference at all. Another way to convey the size of the effect is to compare the difference between groups to the variability within groups. Cohen’s \\(d\\) is a statistic intended to do that; it is defined \\[ d = \\frac{\\bar x_1 - \\bar x_2}{s} \\] where \\(\\bar x_1\\) and \\(\\bar x_2\\) are the means of the groups and \\(s\\) is the “pooled standard deviation”. Here’s the Python code that computes Cohen’s \\(d\\): def CohenEffectSize(group1, group2): diff = group1.mean() - group2.mean() var1 = group1.var() var2 = group2.var() n1, n2 = len(group1), len(group2) pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2) d = diff / math.sqrt(pooled_var) return d In this example, the difference in means is 0.029 standard deviations, which is small. To put that in perspective, the difference in height between men and women is about 1.7 standard deviations (see https://en.wikipedia.org/wiki/Effect_size). 2.10 Reporting Results We have seen several ways to describe the difference in pregnancy length (if there is one) between first babies and others. How should we report these results? The answer depends on who is asking the question. A scientist might be interested in any (real) effect, no matter how small. A doctor might only care about effects that are clinically significant; that is, differences that affect treatment decisions. A pregnant woman might be interested in results that are relevant to her, like the probability of delivering early or late. How you report results also depends on your goals. If you are trying to demonstrate the importance of an effect, you might choose summary statistics that emphasize differences. If you are trying to reassure a patient, you might choose statistics that put the differences in context. Of course your decisions should also be guided by professional ethics. It’s ok to be persuasive; you should design statistical reports and visualizations that tell a story clearly. But you should also do your best to make your reports honest, and to acknowledge uncertainty and limitations. 2.11 Exercises Exercise 1 Based on the results in this chapter, suppose you were asked to summarize what you learned about whether first babies arrive late. Which summary statistics would you use if you wanted to get a story on the evening news? Which ones would you use if you wanted to reassure an anxious patient? Finally, imagine that you are Cecil Adams, author of The Straight Dope (http://straightdope.com), and your job is to answer the question, “Do first babies arrive late?” Write a paragraph that uses the results in this chapter to answer the question clearly, precisely, and honestly. Exercise 2 In the repository you downloaded, you should find a file named chap02ex.ipynb; open it. Some cells are already filled in, and you should execute them. Other cells give you instructions for exercises. Follow the instructions and fill in the answers. A solution to this exercise is in chap02soln.ipynb In the repository you downloaded, you should find a file named chap02ex.py; you can use this file as a starting place for the following exercises. My solution is in chap02soln.py. Exercise 3 The mode of a distribution is the most frequent value; see http://wikipedia.org/wiki/Mode_(statistics). Write a function called Mode that takes a Hist and returns the most frequent value. As a more challenging exercise, write a function called AllModes that returns a list of value-frequency pairs in descending order of frequency. Exercise 4 Using the variable totalwgt_lb, investigate whether first babies are lighter or heavier than others. Compute Cohen’s d to quantify the difference between the groups. How does it compare to the difference in pregnancy length? 2.12 Glossary distribution: The values that appear in a sample and the frequency of each. histogram: A mapping from values to frequencies, or a graph that shows this mapping. frequency: The number of times a value appears in a sample. mode: The most frequent value in a sample, or one of the most frequent values. normal distribution: An idealization of a bell-shaped distribution; also known as a Gaussian distribution. uniform distribution: A distribution in which all values have the same frequency. tail: The part of a distribution at the high and low extremes. central tendency: A characteristic of a sample or population; intuitively, it is an average or typical value. outlier: A value far from the central tendency. spread: A measure of how spread out the values in a distribution are. summary statistic: A statistic that quantifies some aspect of a distribution, like central tendency or spread. variance: A summary statistic often used to quantify spread. standard deviation: The square root of variance, also used as a measure of spread. effect size: A summary statistic intended to quantify the size of an effect like a difference between groups. clinically significant: A result, like a difference between groups, that is relevant in practice. "],
["probability.html", "Chapter 3 Probability Mass Functions 3.1 PMFs 3.2 Plotting PMFs 3.3 Other Visualizations 3.4 The Class Size Paradox 3.5 DataFrame Indexing 3.6 Exercises 3.7 Glossary", " Chapter 3 Probability Mass Functions The code for this chapter is in probability.py. For information about downloading and working with this code, see Section 0.2. 3.1 PMFs Another way to represent a distribution is a probability mass function (PMF), which maps from each value to its probability. A probability is a frequency expressed as a fraction of the sample size, n. To get from frequencies to probabilities, we divide through by n, which is called normalization. Given a Hist, we can make a dictionary that maps from each value to its probability: n = hist.Total() d = {} for x, freq in hist.Items(): d[x] = freq / n Or we can use the Pmf class provided by thinkstats2. Like Hist, the Pmf constructor can take a list, pandas Series, dictionary, Hist, or another Pmf object. Here’s an example with a simple list: &gt;&gt;&gt; import thinkstats2 &gt;&gt;&gt; pmf = thinkstats2.Pmf([1, 2, 2, 3, 5]) &gt;&gt;&gt; pmf Pmf({1: 0.2, 2: 0.4, 3: 0.2, 5: 0.2}) The Pmf is normalized so total probability is 1. Pmf and Hist objects are similar in many ways; in fact, they inherit many of their methods from a common parent class. For example, the methods Values and Items work the same way for both. The biggest difference is that a Hist maps from values to integer counters; a Pmf maps from values to floating-point probabilities. To look up the probability associated with a value, use Prob: &gt;&gt;&gt; pmf.Prob(2) 0.4 The bracket operator is equivalent: &gt;&gt;&gt; pmf[2] 0.4 You can modify an existing Pmf by incrementing the probability associated with a value: &gt;&gt;&gt; pmf.Incr(2, 0.2) &gt;&gt;&gt; pmf.Prob(2) 0.6 Or you can multiply a probability by a factor: &gt;&gt;&gt; pmf.Mult(2, 0.5) &gt;&gt;&gt; pmf.Prob(2) 0.3 If you modify a Pmf, the result may not be normalized; that is, the probabilities may no longer add up to 1. To check, you can call Total, which returns the sum of the probabilities: &gt;&gt;&gt; pmf.Total() 0.9 To renormalize, call Normalize: &gt;&gt;&gt; pmf.Normalize() &gt;&gt;&gt; pmf.Total() 1.0 Pmf objects provide a Copy method so you can make and modify a copy without affecting the original. My notation in this section might seem inconsistent, but there is a system: I use Pmf for the name of the class, pmf for an instance of the class, and PMF for the mathematical concept of a probability mass function. 3.2 Plotting PMFs thinkplot provides two ways to plot Pmfs: To plot a Pmf as a bar graph, you can use thinkplot.Hist. Bar graphs are most useful if the number of values in the Pmf is small. To plot a Pmf as a step function, you can use thinkplot.Pmf. This option is most useful if there are a large number of values and the Pmf is smooth. This function also works with Hist objects. In addition, pyplot provides a function called hist that takes a sequence of values, computes a histogram, and plots it. Since I use Hist objects, I usually don’t use pyplot.hist. Figure 3.1: PMF of pregnancy lengths for first babies and others, using bar graphs and step functions. Figure 3.1 shows PMFs of pregnancy length for first babies and others using bar graphs (left) and step functions (right). By plotting the PMF instead of the histogram, we can compare the two distributions without being mislead by the difference in sample size. Based on this figure, first babies seem to be less likely than others to arrive on time (week 39) and more likely to be a late (weeks 41 and 42). Here’s the code that generates Figure 3.1 thinkplot.PrePlot(2, cols=2) thinkplot.Hist(first_pmf, align=&#39;right&#39;, width=width) thinkplot.Hist(other_pmf, align=&#39;left&#39;, width=width) thinkplot.Config(xlabel=&#39;weeks&#39;, ylabel=&#39;probability&#39;, axis=[27, 46, 0, 0.6]) thinkplot.PrePlot(2) thinkplot.SubPlot(2) thinkplot.Pmfs([first_pmf, other_pmf]) thinkplot.Show(xlabel=&#39;weeks&#39;, axis=[27, 46, 0, 0.6]) PrePlot takes optional parameters rows and cols to make a grid of figures, in this case one row of two figures. The first figure (on the left) displays the Pmfs using thinkplot.Hist, as we have seen before. The second call to PrePlot resets the color generator. Then SubPlot switches to the second figure (on the right) and displays the Pmfs using thinkplot.Pmfs. I used the axis option to ensure that the two figures are on the same axes, which is generally a good idea if you intend to compare two figures. 3.3 Other Visualizations Histograms and PMFs are useful while you are exploring data and trying to identify patterns and relationships. Once you have an idea what is going on, a good next step is to design a visualization that makes the patterns you have identified as clear as possible. In the NSFG data, the biggest differences in the distributions are near the mode. So it makes sense to zoom in on that part of the graph, and to transform the data to emphasize differences: weeks = range(35, 46) diffs = [] for week in weeks: p1 = first_pmf.Prob(week) p2 = other_pmf.Prob(week) diff = 100 * (p1 - p2) diffs.append(diff) thinkplot.Bar(weeks, diffs) In this code, weeks is the range of weeks; diffs is the difference between the two PMFs in percentage points. Figure 3.2 shows the result as a bar chart. This figure makes the pattern clearer: first babies are less likely to be born in week 39, and somewhat more likely to be born in weeks 41 and 42. Figure 3.2: Difference, in percentage points, by week. For now we should hold this conclusion only tentatively. We used the same dataset to identify an apparent difference and then chose a visualization that makes the difference apparent. We can’t be sure this effect is real; it might be due to random variation. We’ll address this concern later. 3.4 The Class Size Paradox Before we go on, I want to demonstrate one kind of computation you can do with Pmf objects; I call this example the “class size paradox.” At many American colleges and universities, the student-to-faculty ratio is about 10:1. But students are often surprised to discover that their average class size is bigger than 10. There are two reasons for the discrepancy: Students typically take 4–5 classes per semester, but professors often teach 1 or 2. The number of students who enjoy a small class is small, but the number of students in a large class is (ahem!) large. The first effect is obvious, at least once it is pointed out; the second is more subtle. Let’s look at an example. Suppose that a college offers 65 classes in a given semester, with the following distribution of sizes: size count 5- 9 8 10-14 8 15-19 14 20-24 4 25-29 6 30-34 12 35-39 8 40-44 3 45-49 2 If you ask the Dean for the average class size, he would construct a PMF, compute the mean, and report that the average class size is 23.7. Here’s the code: d = { 7: 8, 12: 8, 17: 14, 22: 4, 27: 6, 32: 12, 37: 8, 42: 3, 47: 2 } pmf = thinkstats2.Pmf(d, label=&#39;actual&#39;) print(&#39;mean&#39;, pmf.Mean()) But if you survey a group of students, ask them how many students are in their classes, and compute the mean, you would think the average class was bigger. Let’s see how much bigger. First, I compute the distribution as observed by students, where the probability associated with each class size is “biased” by the number of students in the class. def BiasPmf(pmf, label): new_pmf = pmf.Copy(label=label) for x, p in pmf.Items(): new_pmf.Mult(x, x) new_pmf.Normalize() return new_pmf For each class size, x, we multiply the probability by x, the number of students who observe that class size. The result is a new Pmf that represents the biased distribution. Now we can plot the actual and observed distributions: biased_pmf = BiasPmf(pmf, label=&#39;observed&#39;) thinkplot.PrePlot(2) thinkplot.Pmfs([pmf, biased_pmf]) thinkplot.Show(xlabel=&#39;class size&#39;, ylabel=&#39;PMF&#39;) Figure 3.3: Distribution of class sizes, actual and as observed by students. Figure 3.3 shows the result. In the biased distribution there are fewer small classes and more large ones. The mean of the biased distribution is 29.1, almost 25% higher than the actual mean. It is also possible to invert this operation. Suppose you want to find the distribution of class sizes at a college, but you can’t get reliable data from the Dean. An alternative is to choose a random sample of students and ask how many students are in their classes. The result would be biased for the reasons we’ve just seen, but you can use it to estimate the actual distribution. Here’s the function that unbiases a Pmf: def UnbiasPmf(pmf, label): new_pmf = pmf.Copy(label=label) for x, p in pmf.Items(): new_pmf.Mult(x, 1.0/x) new_pmf.Normalize() return new_pmf It’s similar to BiasPmf; the only difference is that it divides each probability by x instead of multiplying. 3.5 DataFrame Indexing In Section 1.4 we read a pandas DataFrame and used it to select and modify data columns. Now let’s look at row selection. To start, I create a NumPy array of random numbers and use it to initialize a DataFrame: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; import pandas &gt;&gt;&gt; array = np.random.randn(4, 2) &gt;&gt;&gt; df = pandas.DataFrame(array) &gt;&gt;&gt; df 0 1 0 -0.143510 0.616050 1 -1.489647 0.300774 2 -0.074350 0.039621 3 -1.369968 0.545897 By default, the rows and columns are numbered starting at zero, but you can provide column names: &gt;&gt;&gt; columns = [&#39;A&#39;, &#39;B&#39;] &gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns) &gt;&gt;&gt; df A B 0 -0.143510 0.616050 1 -1.489647 0.300774 2 -0.074350 0.039621 3 -1.369968 0.545897 You can also provide row names. The set of row names is called the index the row names themselves are called labels. &gt;&gt;&gt; index = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] &gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns, index=index) &gt;&gt;&gt; df A B a -0.143510 0.616050 b -1.489647 0.300774 c -0.074350 0.039621 d -1.369968 0.545897 As we saw in the previous chapter, simple indexing selects a column, returning a Series: &gt;&gt;&gt; df[&#39;A&#39;] a -0.143510 b -1.489647 c -0.074350 d -1.369968 Name: A, dtype: float64 To select a row by label, you can use the loc attribute, which returns a Series: &gt;&gt;&gt; df.loc[&#39;a&#39;] A -0.14351 B 0.61605 Name: a, dtype: float64 If you know the integer position of a row, rather than its label, you can use the iloc attribute, which also returns a Series. &gt;&gt;&gt; df.iloc[0] A -0.14351 B 0.61605 Name: a, dtype: float64 loc can also take a list of labels; in that case, the result is a DataFrame. &gt;&gt;&gt; indices = [&#39;a&#39;, &#39;c&#39;] &gt;&gt;&gt; df.loc[indices] A B a -0.14351 0.616050 c -0.07435 0.039621 Finally, you can use a slice to select a range of rows by label: &gt;&gt;&gt; df[&#39;a&#39;:&#39;c&#39;] A B a -0.143510 0.616050 b -1.489647 0.300774 c -0.074350 0.039621 Or by integer position: &gt;&gt;&gt; df[0:2] A B a -0.143510 0.616050 b -1.489647 0.300774 The result in either case is a DataFrame, but notice that the first result includes the end of the slice; the second doesn’t. My advice: if your rows have labels that are not simple integers, use the labels consistently and avoid using integer positions. 3.6 Exercises Solutions to these exercises are in chap03soln.ipynb and chap03soln.py Exercise 1 Something like the class size paradox appears if you survey children and ask how many children are in their family. Families with many children are more likely to appear in your sample, and families with no children have no chance to be in the sample. Use the NSFG respondent variable NUMKDHH to construct the actual distribution for the number of children under 18 in the household. Now compute the biased distribution we would see if we surveyed the children and asked them how many children under 18 (including themselves) are in their household. Plot the actual and biased distributions, and compute their means. As a starting place, you can use chap03ex.ipynb. Exercise 2 In Section 2.7 we computed the mean of a sample by adding up the elements and dividing by n. If you are given a PMF, you can still compute the mean, but the process is slightly different: \\[ \\bar x = \\sum_i p_ix_i \\] where the \\(x_i\\) are the unique values in the PMF and $p_i=PMF(x_i). Similarly, you can compute variance like this: \\[ S^2 = \\sum_i p_i (x_i - \\bar x)^2 \\] Write functions called PmfMean and PmfVar that take a Pmf object and compute the mean and variance. To test these methods, check that they are consistent with the methods Mean and Var provided by Pmf. Exercise 3 I started with the question, “Are first babies more likely to be late?” To address it, I computed the difference in means between groups of babies, but I ignored the possibility that there might be a difference between first babies and others for the same woman. To address this version of the question, select respondents who have at least two babies and compute pairwise differences. Does this formulation of the question yield a different result? Hint: use nsfg.MakePregMap. Exercise 4 In most foot races, everyone starts at the same time. If you are a fast runner, you usually pass a lot of people at the beginning of the race, but after a few miles everyone around you is going at the same speed. When I ran a long-distance (209 miles) relay race for the first time, I noticed an odd phenomenon: when I overtook another runner, I was usually much faster, and when another runner overtook me, he was usually much faster. At first I thought that the distribution of speeds might be bimodal; that is, there were many slow runners and many fast runners, but few at my speed. Then I realized that I was the victim of a bias similar to the effect of class size. The race was unusual in two ways: it used a staggered start, so teams started at different times; also, many teams included runners at different levels of ability. As a result, runners were spread out along the course with little relationship between speed and location. When I joined the race, the runners near me were (pretty much) a random sample of the runners in the race. So where does the bias come from? During my time on the course, the chance of overtaking a runner, or being overtaken, is proportional to the difference in our speeds. I am more likely to catch a slow runner, and more likely to be caught by a fast runner. But runners at the same speed are unlikely to see each other. Write a function called ObservedPmf that takes a Pmf representing the actual distribution of runners’ speeds, and the speed of a running observer, and returns a new Pmf representing the distribution of runners’ speeds as seen by the observer. To test your function, you can use relay.py, which reads the results from the James Joyce Ramble 10K in Dedham MA and converts the pace of each runner to mph. Compute the distribution of speeds you would observe if you ran a relay race at 7.5 mph with this group of runners. A solution to this exercise is in relay_soln.py. 3.7 Glossary Probability Mass Function (PMF): a representation of a distribution as a function that maps from values to probabilities. probability: A frequency expressed as a fraction of the sample size. normalization: The process of dividing a frequency by a sample size to get a probability. index: In a pandas DataFrame, the index is a special column that contains the row labels. "],
["cumulative.html", "Chapter 4 Cumulative Distribution Functions 4.1 The Limits of PMFs 4.2 Percentiles 4.3 CDFs 4.4 Representing CDFs 4.5 Comparing CDFs 4.6 Percentile-based Statistics 4.7 Random Numbers 4.8 Comparing Percentile Ranks 4.9 Exercises 4.10 Glossary", " Chapter 4 Cumulative Distribution Functions The code for this chapter is in cumulative.py. For information about downloading and working with this code, see Section 0.2. 4.1 The Limits of PMFs PMFs work well if the number of values is small. But as the number of values increases, the probability associated with each value gets smaller and the effect of random noise increases. For example, we might be interested in the distribution of birth weights. In the NSFG data, the variable totalwgt_lb records weight at birth in pounds. Figure 4.1 shows the PMF of these values for first babies and others. Figure 4.1: PMF of birth weights. This figure shows a limitation of PMFs: they are hard to compare visually. Overall, these distributions resemble the bell shape of a normal distribution, with many values near the mean and a few values much higher and lower. But parts of this figure are hard to interpret. There are many spikes and valleys, and some apparent differences between the distributions. It is hard to tell which of these features are meaningful. Also, it is hard to see overall patterns; for example, which distribution do you think has the higher mean? These problems can be mitigated by binning the data; that is, dividing the range of values into non-overlapping intervals and counting the number of values in each bin. Binning can be useful, but it is tricky to get the size of the bins right. If they are big enough to smooth out noise, they might also smooth out useful information. An alternative that avoids these problems is the cumulative distribution function (CDF), which is the subject of this chapter. But before I can explain CDFs, I have to explain percentiles. 4.2 Percentiles If you have taken a standardized test, you probably got your results in the form of a raw score and a percentile rank. In this context, the percentile rank is the fraction of people who scored lower than you (or the same). So if you are “in the 90th percentile,” you did as well as or better than 90% of the people who took the exam. Here’s how you could compute the percentile rank of a value, your_score, relative to the values in the sequence scores: def PercentileRank(scores, your_score): count = 0 for score in scores: if score &lt;= your_score: count += 1 percentile_rank = 100.0 * count / len(scores) return percentile_rank As an example, if the scores in the sequence were 55, 66, 77, 88 and 99, and you got the 88, then your percentile rank would be 100 * 4 / 5 which is 80. If you are given a value, it is easy to find its percentile rank; going the other way is slightly harder. If you are given a percentile rank and you want to find the corresponding value, one option is to sort the values and search for the one you want: def Percentile(scores, percentile_rank): scores.sort() for score in scores: if PercentileRank(scores, score) &gt;= percentile_rank: return score The result of this calculation is a percentile. For example, the 50th percentile is the value with percentile rank 50. In the distribution of exam scores, the 50th percentile is 77. This implementation of Percentile is not efficient. A better approach is to use the percentile rank to compute the index of the corresponding percentile: def Percentile2(scores, percentile_rank): scores.sort() index = percentile_rank * (len(scores)-1) // 100 return scores[index] The difference between “percentile” and “percentile rank” can be confusing, and people do not always use the terms precisely. To summarize, PercentileRank takes a value and computes its percentile rank in a set of values; Percentile takes a percentile rank and computes the corresponding value. 4.3 CDFs Now that we understand percentiles and percentile ranks, we are ready to tackle the cumulative distribution function (CDF). The CDF is the function that maps from a value to its percentile rank. The CDF is a function of \\(x\\), where \\(x\\) is any value that might appear in the distribution. To evaluate \\(CDF(x)\\) for a particular value of \\(x\\), we compute the fraction of values in the distribution less than or equal to \\(x\\). Here’s what that looks like as a function that takes a sequence, sample, and a value, x: def EvalCdf(sample, x): count = 0.0 for value in sample: if value &lt;= x: count += 1 prob = count / len(sample) return prob This function is almost identical to PercentileRank, except that the result is a probability in the range 0–1 rather than a percentile rank in the range 0–100. As an example, suppose we collect a sample with the values [1, 2, 2, 3, 5]. Here are some values from its CDF: \\[ CDF(0) = 0 \\] \\[ CDF(1) = 0.2 \\] \\[ CDF(2) = 0.6 \\] \\[ CDF(3) = 0.8 \\] \\[ CDF(4) = 0.8 \\] \\[ CDF(5) = 1 \\] We can evaluate the CDF for any value of \\(x\\), not just values that appear in the sample. If \\(x\\) is less than the smallest value in the sample, \\(CDF(x)\\) is 0. If \\(x\\) is greater than the largest value, \\(CDF(x)\\) is 1. Figure 4.2: Example of a CDF. Figure 4.2 is a graphical representation of this CDF. The CDF of a sample is a step function. 4.4 Representing CDFs thinkstats2 provides a class named Cdf that represents CDFs. The fundamental methods Cdf provides are: Prob(x): Given a value x, computes the probability \\(p = CDF(x)\\). The bracket operator is equivalent to Prob Value(p): Given a probability p, computes the corresponding value, x; that is, the inverse CDF of p. Figure 4.3: CDF of pregnancy length. The Cdf constructor can take as an argument a list of values, a pandas Series, a Hist, Pmf, or another Cdf. The following code makes a Cdf for the distribution of pregnancy lengths in the NSFG: live, firsts, others = first.MakeFrames() cdf = thinkstats2.Cdf(live.prglngth, label=&#39;prglngth&#39;) thinkplot provides a function named Cdf that plots Cdfs as lines: thinkplot.Cdf(cdf) thinkplot.Show(xlabel=&#39;weeks&#39;, ylabel=&#39;CDF&#39;) Figure 4.3 shows the result. One way to read a CDF is to look up percentiles. For example, it looks like about 10% of pregnancies are shorter than 36 weeks, and about 90% are shorter than 41 weeks. The CDF also provides a visual representation of the shape of the distribution. Common values appear as steep or vertical sections of the CDF; in this example, the mode at 39 weeks is apparent. There are few values below 30 weeks, so the CDF in this range is flat. It takes some time to get used to CDFs, but once you do, I think you will find that they show more information, more clearly, than PMFs. 4.5 Comparing CDFs CDFs are especially useful for comparing distributions. For example, here is the code that plots the CDF of birth weight for first babies and others. first_cdf = thinkstats2.Cdf(firsts.totalwgt_lb, label=&#39;first&#39;) other_cdf = thinkstats2.Cdf(others.totalwgt_lb, label=&#39;other&#39;) thinkplot.PrePlot(2) thinkplot.Cdfs([first_cdf, other_cdf]) thinkplot.Show(xlabel=&#39;weight (pounds)&#39;, ylabel=&#39;CDF&#39;) Figure 4.4: CDF of birth weights for first babies and others. Figure 4.4 shows the result. Compared to Figure 4.1, this figure makes the shape of the distributions, and the differences between them, much clearer. We can see that first babies are slightly lighter throughout the distribution, with a larger discrepancy above the mean. 4.6 Percentile-based Statistics Once you have computed a CDF, it is easy to compute percentiles and percentile ranks. The Cdf class provides these two methods: PercentileRank(x): Given a value x, computes its percentile rank, \\(100 * CDF(x)\\). Percentile(p): Given a percentile rank p, computes the corresponding value, x. Equivalent to Value(p/100). Percentile can be used to compute percentile-based summary statistics. For example, the 50th percentile is the value that divides the distribution in half, also known as the median. Like the mean, the median is a measure of the central tendency of a distribution. Actually, there are several definitions of “median,” each with different properties. But Percentile(50) is simple and efficient to compute. Another percentile-based statistic is the interquartile range (IQR), which is a measure of the spread of a distribution. The IQR is the difference between the 75th and 25th percentiles. More generally, percentiles are often used to summarize the shape of a distribution. For example, the distribution of income is often reported in “quintiles”; that is, it is split at the 20th, 40th, 60th and 80th percentiles. Other distributions are divided into ten “deciles”. Statistics like these that represent equally-spaced points in a CDF are called quantiles. For more, see https://en.wikipedia.org/wiki/Quantile 4.7 Random Numbers Suppose we choose a random sample from the population of live births and look up the percentile rank of their birth weights. Now suppose we compute the CDF of the percentile ranks. What do you think the distribution will look like? Here’s how we can compute it. First, we make the Cdf of birth weights: weights = live.totalwgt_lb cdf = thinkstats2.Cdf(weights, label=&#39;totalwgt_lb&#39;) Then we generate a sample and compute the percentile rank of each value in the sample. sample = np.random.choice(weights, 100, replace=True) ranks = [cdf.PercentileRank(x) for x in sample] sample is a random sample of 100 birth weights, chosen with replacement; that is, the same value could be chosen more than once. ranks is a list of percentile ranks. Finally we make and plot the Cdf of the percentile ranks. rank_cdf = thinkstats2.Cdf(ranks) thinkplot.Cdf(rank_cdf) thinkplot.Show(xlabel=&#39;percentile rank&#39;, ylabel=&#39;CDF&#39;) Figure 4.5: CDF of percentile ranks for a random sample of birth weights. Figure 4.5 shows the result. The CDF is approximately a straight line, which means that the distribution is uniform. That outcome might be non-obvious, but it is a consequence of the way the CDF is defined. What this figure shows is that 10% of the sample is below the 10th percentile, 20% is below the 20th percentile, and so on, exactly as we should expect. So, regardless of the shape of the CDF, the distribution of percentile ranks is uniform. This property is useful, because it is the basis of a simple and efficient algorithm for generating random numbers with a given CDF. Here’s how: Choose a percentile rank uniformly from the range 0–100. Use Cdf.Percentile to find the value in the distribution that corresponds to the percentile rank you chose. Cdf provides an implementation of this algorithm, called Random: # class Cdf: def Random(self): return self.Percentile(random.uniform(0, 100)) Cdf also provides Sample, which takes an integer,n, and returns a list of n values chosen at random from the Cdf. 4.8 Comparing Percentile Ranks Percentile ranks are useful for comparing measurements across different groups. For example, people who compete in foot races are usually grouped by age and gender. To compare people in different age groups, you can convert race times to percentile ranks. A few years ago I ran the James Joyce Ramble 10K in Dedham MA; I finished in 42:44, which was 97th in a field of 1633. I beat or tied 1537 runners out of 1633, so my percentile rank in the field is 94%. More generally, given position and field size, we can compute percentile rank: def PositionToPercentile(position, field_size): beat = field_size - position + 1 percentile = 100.0 * beat / field_size return percentile In my age group, denoted M4049 for “male between 40 and 49 years of age”, I came in 26th out of 256. So my percentile rank in my age group was 90%. If I am still running in 10 years (and I hope I am), I will be in the M5059 division. Assuming that my percentile rank in my division is the same, how much slower should I expect to be? I can answer that question by converting my percentile rank in M4049 to a position in M5059. Here’s the code: def PercentileToPosition(percentile, field_size): beat = percentile * field_size / 100.0 position = field_size - beat + 1 return position There were 171 people in M5059, so I would have to come in between 17th and 18th place to have the same percentile rank. The finishing time of the 17th runner in M5059 was 46:05, so that’s the time I will have to beat to maintain my percentile rank. 4.9 Exercises For the following exercises, you can start with chap04ex.ipynb. My solution is in chap04soln.ipynb. Exercise 1 How much did you weigh at birth? If you don’t know, call your mother or someone else who knows. Using the NSFG data (all live births), compute the distribution of birth weights and use it to find your percentile rank. If you were a first baby, find your percentile rank in the distribution for first babies. Otherwise use the distribution for others. If you are in the 90th percentile or higher, call your mother back and apologize. Exercise 2 The numbers generated by random.random are supposed to be uniform between 0 and 1; that is, every value in the range should have the same probability. Generate 1000 numbers from random.random and plot their PMF and CDF. Is the distribution uniform? 4.10 Glossary percentile rank: The percentage of values in a distribution that are less than or equal to a given value. percentile: The value associated with a given percentile rank. cumulative distribution function (CDF): A function that maps from values to their cumulative probabilities. \\(CDF(x)\\) is the fraction of the sample less than or equal to \\(x\\). inverse CDF: A function that maps from a cumulative probability, \\(p\\), to the corresponding value. median: The 50th percentile, often used as a measure of central tendency. interquartile range: The difference between the 75th and 25th percentiles, used as a measure of spread. quantile: A sequence of values that correspond to equally spaced percentile ranks; for example, the quartiles of a distribution are the 25th, 50th and 75th percentiles. replacement: A property of a sampling process. “With replacement” means that the same value can be chosen more than once; “without replacement” means that once a value is chosen, it is removed from the population. "]
]
