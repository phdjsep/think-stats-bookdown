<td valign="top" width="600">

<p>This HTML version of is provided for convenience, but it
is not the best format for the book.  In particular, some of the
symbols are not rendered correctly.

</p>
<p>You might prefer to read
the <a href="http://thinkstats2.com/thinkstats2.pdf">PDF version</a>, or
you can buy a hardcopy from
<a href="http://amzn.to/2gBBW7v">Amazon</a>.
</p>
<h1 id="sec80" class="chapter"><span style="font-size:medium">Chapter 8  Estimation</span></h1>
<p><span style="font-size:medium">
</span><a id="estimation"></a><span style="font-size:medium">
</span><a id="hevea_default718"></a></p>
<p><span style="font-size:medium">The code for this chapter is in <span style="font-family:monospace">estimation.py</span>. For information
about downloading and working with this code, see Section </span><a href="thinkstats2001.html#code"><span style="font-size:medium">0.2</span></a><span style="font-size:medium">.</span></p>
<span style="font-size:medium">
</span><h2 id="sec81" class="section"><span style="font-size:medium">8.1  The estimation game</span></h2>
<p><span style="font-size:medium">Let’s play a game. I think of a distribution, and you have to guess
what it is. I’ll give you two hints: it’s a
normal distribution, and here’s a random sample drawn from it:
</span><a id="hevea_default719"></a><span style="font-size:medium">
</span><a id="hevea_default720"></a><span style="font-size:medium">
</span><a id="hevea_default721"></a><span style="font-size:medium">
</span><a id="hevea_default722"></a></p>
<p><span style="font-family:monospace;font-size:medium">[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]</span></p>
<p><span style="font-size:medium">What do you think is the mean parameter, µ, of this distribution?
</span><a id="hevea_default723"></a><span style="font-size:medium">
</span><a id="hevea_default724"></a></p>
<p><span style="font-size:medium">One choice is to use the sample mean, <span style="text-decoration:overline">x</span>, as an estimate of µ.
In this example, <span style="text-decoration:overline">x</span> is 0.155, so it would
be reasonable to guess µ = 0.155.
This process is called <span style="font-weight:bold">estimation</span>, and the statistic we used
(the sample mean) is called an <span style="font-weight:bold">estimator</span>.
</span><a id="hevea_default725"></a></p>
<p><span style="font-size:medium">Using the sample mean to estimate µ is so obvious that it is hard
to imagine a reasonable alternative. But suppose we change the game by
introducing outliers.
</span><a id="hevea_default726"></a><span style="font-size:medium">
</span><a id="hevea_default727"></a><span style="font-size:medium">
</span><a id="hevea_default728"></a><span style="font-size:medium">
</span><a id="hevea_default729"></a></p>
<p><span style="font-size:medium"><em>I’m thinking of a distribution.</em> It’s a normal distribution, and
here’s a sample that was collected by an unreliable surveyor who
occasionally puts the decimal point in the wrong place.
</span><a id="hevea_default730"></a></p>
<p><span style="font-family:monospace;font-size:medium">[-0.441, 1.774, -0.101, -1.138, 2.975, -213.8]</span></p>
<p><span style="font-size:medium">Now what’s your estimate of µ? If you use the sample mean, your
guess is -35.12. Is that the best choice? What are the alternatives?
</span><a id="hevea_default731"></a></p>
<p><span style="font-size:medium">One option is to identify and discard outliers, then compute the sample
mean of the rest. Another option is to use the median as an estimator.
</span><a id="hevea_default732"></a></p>
<p><span style="font-size:medium">Which estimator is best depends on the circumstances (for example,
whether there are outliers) and on what the goal is. Are you
trying to minimize errors, or maximize your chance of getting the
right answer?
</span><a id="hevea_default733"></a><span style="font-size:medium">
</span><a id="hevea_default734"></a><span style="font-size:medium">
</span><a id="hevea_default735"></a></p>
<p><span style="font-size:medium">If there are no outliers, the sample mean minimizes the <span style="font-weight:bold">mean squared
error</span> (MSE). That is, if we play the game many times, and each time
compute the error <span style="text-decoration:overline">x</span> − µ, the sample mean minimizes
</span></p>
<table class="display dcenter"><tr style="vertical-align:middle">
<td class="dcell"><span style="font-size:medium"><span style="font-style:italic">MSE</span> = </span></td>
<td class="dcell"><table class="display">
<tr><td class="dcell" style="text-align:center"><span style="font-size:medium">1</span></td></tr>
<tr><td class="hbar"></td></tr>
<tr><td class="dcell" style="text-align:center"><span style="font-style:italic;font-size:medium">m</span></td></tr>
</table></td>
<td class="dcell">
<span style="font-size:medium"> <span style="font-size:xx-large">∑</span>(<span style="text-decoration:overline">x</span> − µ)</span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> </span>
</td>
</tr></table>
<p><span style="font-size:medium">
Where <span style="font-style:italic">m</span> is the number of times you play the estimation game, not
to be confused with <span style="font-style:italic">n</span>, which is the size of the sample used to
compute <span style="text-decoration:overline">x</span>.</span></p>
<p><span style="font-size:medium">Here is a function that simulates the estimation game and computes
the root mean squared error (RMSE), which is the square root of
MSE:
</span><a id="hevea_default736"></a><span style="font-size:medium">
</span><a id="hevea_default737"></a><span style="font-size:medium">
</span><a id="hevea_default738"></a></p>
<pre class="verbatim"><span style="font-size:medium">def Estimate1(n=7, m=1000):
    mu = 0
    sigma = 1

    means = []
    medians = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        xbar = np.mean(xs)
        median = np.median(xs)
        means.append(xbar)
        medians.append(median)

    print('rmse xbar', RMSE(means, mu))
    print('rmse median', RMSE(medians, mu))
</span></pre>
<p><span style="font-size:medium">Again, <span style="font-family:monospace">n</span> is the size of the sample, and <span style="font-family:monospace">m</span> is the
number of times we play the game. <span style="font-family:monospace">means</span> is the list of
estimates based on <span style="text-decoration:overline">x</span>. <span style="font-family:monospace">medians</span> is the list of medians.
</span><a id="hevea_default739"></a></p>
<p><span style="font-size:medium">Here’s the function that computes RMSE:</span></p>
<pre class="verbatim"><span style="font-size:medium">def RMSE(estimates, actual):
    e2 = [(estimate-actual)**2 for estimate in estimates]
    mse = np.mean(e2)
    return math.sqrt(mse)
</span></pre>
<p><span style="font-size:medium"><span style="font-family:monospace">estimates</span> is a list of estimates; <span style="font-family:monospace">actual</span> is the
actual value being estimated. In practice, of course, we don’t
know <span style="font-family:monospace">actual</span>; if we did, we wouldn’t have to estimate it.
The purpose of this experiment is to compare the performance of
the two estimators.
</span><a id="hevea_default740"></a></p>
<p><span style="font-size:medium">When I ran this code, the RMSE of the sample mean was 0.41, which
means that if we use <span style="text-decoration:overline">x</span> to estimate the mean of this
distribution, based on a sample with <span style="font-style:italic">n</span>=7, we should expect to be off
by 0.41 on average. Using the median to estimate the mean yields
RMSE 0.53, which confirms that <span style="text-decoration:overline">x</span> yields lower RMSE, at least
for this example.</span></p>
<p><span style="font-size:medium">Minimizing MSE is a nice property, but it’s not always the best
strategy. For example, suppose we are estimating the distribution of
wind speeds at a building site. If the estimate is too high, we might
overbuild the structure, increasing its cost. But if it’s too
low, the building might collapse. Because cost as a function of
error is not symmetric, minimizing MSE is not the best strategy.
</span><a id="hevea_default741"></a><span style="font-size:medium">
</span><a id="hevea_default742"></a><span style="font-size:medium">
</span><a id="hevea_default743"></a></p>
<p><span style="font-size:medium">As another example, suppose I roll three six-sided dice and ask you to
predict the total. If you get it exactly right, you get a prize;
otherwise you get nothing. In this case the value that minimizes MSE
is 10.5, but that would be a bad guess, because the total of three
dice is never 10.5. For this game, you want an estimator that has the
highest chance of being right, which is a <span style="font-weight:bold">maximum likelihood
estimator</span> (MLE). If you pick 10 or 11, your chance of winning is 1
in 8, and that’s the best you can do. </span><a id="hevea_default744"></a><span style="font-size:medium">
</span><a id="hevea_default745"></a><span style="font-size:medium">
</span><a id="hevea_default746"></a></p>
<span style="font-size:medium">
</span><h2 id="sec82" class="section"><span style="font-size:medium">8.2  Guess the variance</span></h2>
<p><span style="font-size:medium">
</span><a id="hevea_default747"></a><span style="font-size:medium">
</span><a id="hevea_default748"></a><span style="font-size:medium">
</span><a id="hevea_default749"></a><span style="font-size:medium">
</span><a id="hevea_default750"></a><span style="font-size:medium">
</span><a id="hevea_default751"></a></p>
<p><span style="font-size:medium"><em>I’m thinking of a distribution.</em> It’s a normal distribution, and 
here’s a (familiar) sample:</span></p>
<p><span style="font-family:monospace;font-size:medium">[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]</span></p>
<p><span style="font-size:medium">What do you think is the variance, σ</span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium">, of my distribution?
Again, the obvious choice is to use the sample variance, <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium">, as an
estimator.
</span></p>
<table class="display dcenter"><tr style="vertical-align:middle">
<td class="dcell">
<span style="font-style:italic;font-size:medium">S</span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> = </span>
</td>
<td class="dcell"><table class="display">
<tr><td class="dcell" style="text-align:center"><span style="font-size:medium">1</span></td></tr>
<tr><td class="hbar"></td></tr>
<tr><td class="dcell" style="text-align:center"><span style="font-style:italic;font-size:medium">n</span></td></tr>
</table></td>
<td class="dcell">
<span style="font-size:medium"> <span style="font-size:xx-large">∑</span>(<span style="font-style:italic">x</span></span><sub><span style="font-style:italic;font-size:medium">i</span></sub><span style="font-size:medium"> − <span style="text-decoration:overline">x</span>)</span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> </span>
</td>
</tr></table>
<p><span style="font-size:medium"> 
For large samples, <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> is an adequate estimator, but for small
samples it tends to be too low. Because of this unfortunate
property, it is called a <span style="font-weight:bold">biased</span> estimator.
An estimator is <span style="font-weight:bold">unbiased</span> if the expected total (or mean) error,
after many iterations of the estimation game, is 0.
</span><a id="hevea_default752"></a><span style="font-size:medium">
</span><a id="hevea_default753"></a><span style="font-size:medium">
</span><a id="hevea_default754"></a><span style="font-size:medium">
</span><a id="hevea_default755"></a><span style="font-size:medium">
</span><a id="hevea_default756"></a></p>
<p><span style="font-size:medium">Fortunately, there is another simple statistic that is an unbiased
estimator of σ</span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium">:
</span></p>
<table class="display dcenter"><tr style="vertical-align:middle">
<td class="dcell">
<span style="font-style:italic;font-size:medium">S</span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> = </span>
</td>
<td class="dcell"><table class="display">
<tr><td class="dcell" style="text-align:center"><span style="font-size:medium">1</span></td></tr>
<tr><td class="hbar"></td></tr>
<tr><td class="dcell" style="text-align:center"><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></td></tr>
</table></td>
<td class="dcell">
<span style="font-size:medium"> <span style="font-size:xx-large">∑</span>(<span style="font-style:italic">x</span></span><sub><span style="font-style:italic;font-size:medium">i</span></sub><span style="font-size:medium"> − <span style="text-decoration:overline">x</span>)</span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> </span>
</td>
</tr></table>
<p><span style="font-size:medium"> 
For an explanation of why <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> is biased, and a proof that
<span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> is unbiased, see
</span><a href="http://wikipedia.org/wiki/Bias_of_an_estimator"><span style="font-family:monospace;font-size:medium">http://wikipedia.org/wiki/Bias_of_an_estimator</span></a><span style="font-size:medium">.</span></p>
<p><span style="font-size:medium">The biggest problem with this estimator is that its name and symbol
are used inconsistently. The name “sample variance” can refer to
either <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> or <span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium">, and the symbol <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> is used
for either or both.</span></p>
<p><span style="font-size:medium">Here is a function that simulates the estimation game and tests
the performance of <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> and <span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium">:</span></p>
<pre class="verbatim"><span style="font-size:medium">def Estimate2(n=7, m=1000):
    mu = 0
    sigma = 1

    estimates1 = []
    estimates2 = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        biased = np.var(xs)
        unbiased = np.var(xs, ddof=1)
        estimates1.append(biased)
        estimates2.append(unbiased)

    print('mean error biased', MeanError(estimates1, sigma**2))
    print('mean error unbiased', MeanError(estimates2, sigma**2))
</span></pre>
<p><span style="font-size:medium">Again, <span style="font-family:monospace">n</span> is the sample size and <span style="font-family:monospace">m</span> is the number of times
we play the game. <span style="font-family:monospace">np.var</span> computes <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> by default and
<span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> if you provide the argument <span style="font-family:monospace">ddof=1</span>, which stands for
“delta degrees of freedom.” I won’t explain that term, but you can read
about it at
</span><a href="http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)"><span style="font-family:monospace;font-size:medium">http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)</span></a><span style="font-size:medium">.
</span><a id="hevea_default757"></a></p>
<p><span style="font-size:medium"><span style="font-family:monospace">MeanError</span> computes the mean difference between the estimates
and the actual value:</span></p>
<pre class="verbatim"><span style="font-size:medium">def MeanError(estimates, actual):
    errors = [estimate-actual for estimate in estimates]
    return np.mean(errors)
</span></pre>
<p><span style="font-size:medium">When I ran this code, the mean error for <span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> was -0.13. As
expected, this biased estimator tends to be too low. For <span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium">,
the mean error was 0.014, about 10 times smaller. As <span style="font-family:monospace">m</span>
increases, we expect the mean error for <span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"> to approach 0.
</span><a id="hevea_default758"></a></p>
<p><span style="font-size:medium">Properties like MSE and bias are long-term expectations based on
many iterations of the estimation game. By running simulations like
the ones in this chapter, we can compare estimators and check whether
they have desired properties.
</span><a id="hevea_default759"></a><span style="font-size:medium">
</span><a id="hevea_default760"></a></p>
<p><span style="font-size:medium">But when you apply an estimator to real
data, you just get one estimate. It would not be meaningful to say
that the estimate is unbiased; being unbiased is a property of the
estimator, not the estimate.</span></p>
<p><span style="font-size:medium">After you choose an estimator with appropriate properties, and use it to
generate an estimate, the next step is to characterize the
uncertainty of the estimate, which is the topic of the next
section.</span></p>
<span style="font-size:medium">
</span><h2 id="sec83" class="section"><span style="font-size:medium">8.3  Sampling distributions</span></h2>
<p><span style="font-size:medium">
</span><a id="gorilla"></a></p>
<p><span style="font-size:medium">Suppose you are a scientist studying gorillas in a wildlife
preserve. You want to know the average weight of the adult
female gorillas in the preserve. To weigh them, you have
to tranquilize them, which is dangerous, expensive, and possibly
harmful to the gorillas. But if it is important to obtain this
information, it might be acceptable to weigh a sample of 9
gorillas. Let’s assume that the population of the preserve is
well known, so we can choose a representative sample of adult
females. We could use the sample mean, <span style="text-decoration:overline">x</span>, to estimate the
unknown population mean, µ.
</span><a id="hevea_default761"></a><span style="font-size:medium">
</span><a id="hevea_default762"></a><span style="font-size:medium">
</span><a id="hevea_default763"></a></p>
<p><span style="font-size:medium">Having weighed 9 female gorillas, you might find <span style="text-decoration:overline">x</span>=90 kg and
sample standard deviation, <span style="font-style:italic">S</span>=7.5 kg. The sample mean
is an unbiased estimator of µ, and in the long run it
minimizes MSE. So if you report a single
estimate that summarizes the results, you would report 90 kg.
</span><a id="hevea_default764"></a><span style="font-size:medium">
</span><a id="hevea_default765"></a><span style="font-size:medium">
</span><a id="hevea_default766"></a><span style="font-size:medium">
</span><a id="hevea_default767"></a><span style="font-size:medium">
</span><a id="hevea_default768"></a></p>
<p><span style="font-size:medium">But how confident should you be in this estimate? If you only weigh
<span style="font-style:italic">n</span>=9 gorillas out of a much larger population, you might be unlucky
and choose the 9 heaviest gorillas (or the 9 lightest ones) just by
chance. Variation in the estimate caused by random selection is
called <span style="font-weight:bold">sampling error</span>.
</span><a id="hevea_default769"></a></p>
<p><span style="font-size:medium">To quantify sampling error, we can simulate the
sampling process with hypothetical values of µ and σ, and
see how much <span style="text-decoration:overline">x</span> varies.</span></p>
<p><span style="font-size:medium">Since we don’t know the actual values of 
µ and σ in the population, we’ll use the estimates
<span style="text-decoration:overline">x</span> and <span style="font-style:italic">S</span>.
So the question we answer is:
“If the actual values of µ and σ were 90 kg and 7.5 kg,
and we ran the same experiment many times, how much would the
estimated mean, <span style="text-decoration:overline">x</span>, vary?”</span></p>
<p><span style="font-size:medium">The following function answers that question:</span></p>
<pre class="verbatim"><span style="font-size:medium">def SimulateSample(mu=90, sigma=7.5, n=9, m=1000):
    means = []
    for j in range(m):
        xs = np.random.normal(mu, sigma, n)
        xbar = np.mean(xs)
        means.append(xbar)

    cdf = thinkstats2.Cdf(means)
    ci = cdf.Percentile(5), cdf.Percentile(95)
    stderr = RMSE(means, mu)
</span></pre>
<p><span style="font-size:medium"><span style="font-family:monospace">mu</span> and <span style="font-family:monospace">sigma</span> are the <em>hypothetical</em> values of
the parameters. <span style="font-family:monospace">n</span> is the sample size, the number of
gorillas we measured. <span style="font-family:monospace">m</span> is the number of times we run
the simulation.
</span><a id="hevea_default770"></a><span style="font-size:medium">
</span><a id="hevea_default771"></a><span style="font-size:medium">
</span><a id="hevea_default772"></a></p>
<blockquote class="figure">
<div class="center"><hr style="width:80%;height:2"></div>
<span style="font-size:medium">
</span><div class="center"><span style="font-size:medium"><img src="thinkstats2033.png"></span></div>
<span style="font-size:medium">
</span><div class="caption"><table style="border-spacing:6px;border-collapse:separate;" class="cellpading0"><tr><td style="vertical-align:top;text-align:left;"><span style="font-size:medium">Figure 8.1: Sampling distribution of <span style="text-decoration:overline">x</span>, with confidence interval.</span></td></tr></table></div>
<span style="font-size:medium">
</span><a id="estimation1"></a><span style="font-size:medium">
</span><div class="center"><hr style="width:80%;height:2"></div>
</blockquote>
<p><span style="font-size:medium">In each iteration, we choose <span style="font-family:monospace">n</span> values from a normal
distribution with the given parameters, and compute the sample mean,
<span style="font-family:monospace">xbar</span>. We run 1000 simulations and then compute the
distribution, <span style="font-family:monospace">cdf</span>, of the estimates. The result is shown in
Figure </span><a href="#estimation1"><span style="font-size:medium">8.1</span></a><span style="font-size:medium">. This distribution is called the <span style="font-weight:bold">sampling distribution</span> of the estimator. It shows how much the
estimates would vary if we ran the experiment over and over.
</span><a id="hevea_default773"></a></p>
<p><span style="font-size:medium">The mean of the sampling distribution is pretty close
to the hypothetical value of µ, which means that the experiment
yields the right answer, on average. After 1000 tries, the lowest
result is 82 kg, and the highest is 98 kg. This range suggests that
the estimate might be off by as much as 8 kg.</span></p>
<p><span style="font-size:medium">There are two common ways to summarize the sampling distribution:</span></p>
<ul class="itemize">
<li class="li-itemize">
<span style="font-size:medium"><span style="font-weight:bold">Standard error</span> (SE) is a measure of how far we expect the
estimate to be off, on average. For each simulated experiment, we
compute the error, <span style="text-decoration:overline">x</span> − µ, and then compute the root mean
squared error (RMSE). In this example, it is roughly 2.5 kg.
</span><a id="hevea_default774"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">A <span style="font-weight:bold">confidence interval</span> (CI) is a range that includes a
given fraction of the sampling distribution. For example, the 90%
confidence interval is the range from the 5th to the 95th
percentile. In this example, the 90% CI is (86, 94) kg.
</span><a id="hevea_default775"></a><span style="font-size:medium">
</span><a id="hevea_default776"></a>
</li>
</ul>
<p><span style="font-size:medium">Standard errors and confidence intervals are the source of much confusion:</span></p>
<ul class="itemize">
<li class="li-itemize">
<span style="font-size:medium">People often confuse standard error and standard deviation.
Remember that standard deviation describes variability in a measured
quantity; in this example, the standard deviation of gorilla weight
is 7.5 kg. Standard error describes variability in an estimate. In
this example, the standard error of the mean, based on a sample of 9
measurements, is 2.5 kg.
</span><a id="hevea_default777"></a><span style="font-size:medium">
</span><a id="hevea_default778"></a><p><span style="font-size:medium">One way to remember the difference is that, as sample size
increases, standard error gets smaller; standard deviation does not.</span></p>
</li>
<li class="li-itemize">
<span style="font-size:medium">People often think that there is a 90% probability that the
actual parameter, µ, falls in the 90% confidence interval.
Sadly, that is not true. If you want to make a claim like that, you
have to use Bayesian methods (see my book, <span style="font-style:italic">Think Bayes</span>).
</span><a id="hevea_default779"></a><p><span style="font-size:medium">The sampling distribution answers a different question: it gives you
a sense of how reliable an estimate is by telling you how much it
would vary if you ran the experiment again.
</span><a id="hevea_default780"></a></p>
</li>
</ul>
<p><span style="font-size:medium">It is important to remember that confidence intervals
and standard errors only quantify sampling error; that is,
error due to measuring only part of the population.
The sampling distribution does not account for other
sources of error, notably sampling bias and measurement error, 
which are the topics of the next section.</span></p>
<span style="font-size:medium">
</span><h2 id="sec84" class="section"><span style="font-size:medium">8.4  Sampling bias</span></h2>
<p><span style="font-size:medium">Suppose that instead of the weight of gorillas in a nature preserve,
you want to know the average weight of women in the city where you
live. It is unlikely that you would be allowed
to choose a representative sample of women and
weigh them.
</span><a id="hevea_default781"></a><span style="font-size:medium">
</span><a id="hevea_default782"></a><span style="font-size:medium">
</span><a id="hevea_default783"></a><span style="font-size:medium">
</span><a id="hevea_default784"></a><span style="font-size:medium">
</span><a id="hevea_default785"></a></p>
<p><span style="font-size:medium">A simple alternative would be
“telephone sampling;” that is,
you could choose random numbers from the phone book, call and ask to
speak to an adult woman, and ask how much she weighs.
</span><a id="hevea_default786"></a><span style="font-size:medium">
</span><a id="hevea_default787"></a></p>
<p><span style="font-size:medium">Telephone sampling has obvious limitations. For example, the sample
is limited to people whose telephone numbers are listed, so it
eliminates people without phones (who might be poorer than average)
and people with unlisted numbers (who might be richer). Also, if you
call home telephones during the day, you are less likely to sample
people with jobs. And if you only sample the person who answers the
phone, you are less likely to sample people who share a phone line.</span></p>
<p><span style="font-size:medium">If factors like income, employment, and household size are related
to weight—and it is plausible that they are—the results of your
survey would be affected one way or another. This problem is
called <span style="font-weight:bold">sampling bias</span> because it is a property of the sampling
process.
</span><a id="hevea_default788"></a></p>
<p><span style="font-size:medium">This sampling process is also vulnerable to self-selection, which is a
kind of sampling bias. Some people will refuse to answer the
question, and if the tendency to refuse is related to weight, that
would affect the results.
</span><a id="hevea_default789"></a></p>
<p><span style="font-size:medium">Finally, if you ask people how much they weigh, rather than weighing
them, the results might not be accurate. Even helpful respondents
might round up or down if they are uncomfortable with their actual
weight. And not all respondents are helpful. These inaccuracies are
examples of <span style="font-weight:bold">measurement error</span>.
</span><a id="hevea_default790"></a></p>
<p><span style="font-size:medium">When you report an estimated quantity, it is useful to report
standard error, or a confidence interval, or both, in order to
quantify sampling error. But it is also important to remember that
sampling error is only one source of error, and often it is not the
biggest.
</span><a id="hevea_default791"></a><span style="font-size:medium">
</span><a id="hevea_default792"></a></p>
<span style="font-size:medium">
</span><h2 id="sec85" class="section"><span style="font-size:medium">8.5  Exponential distributions</span></h2>
<p><span style="font-size:medium">
</span><a id="hevea_default793"></a><span style="font-size:medium">
</span><a id="hevea_default794"></a></p>
<p><span style="font-size:medium">Let’s play one more round of the estimation game.
<em>I’m thinking of a distribution.</em> It’s an exponential distribution, and 
here’s a sample:</span></p>
<p><span style="font-family:monospace;font-size:medium">[5.384, 4.493, 19.198, 2.790, 6.122, 12.844]</span></p>
<p><span style="font-size:medium">What do you think is the parameter, λ, of this distribution?
</span><a id="hevea_default795"></a><span style="font-size:medium">
</span><a id="hevea_default796"></a></p>
<p><span style="font-size:medium">In general, the mean of an exponential distribution is 1/λ,
so working backwards, we might choose
</span></p>
<table class="display dcenter"><tr style="vertical-align:middle"><td class="dcell"><span style="font-size:medium"><span style="font-style:italic">L</span> = 1 / <span style="text-decoration:overline">x</span></span></td></tr></table>
<p><span style="font-size:medium">
<span style="font-style:italic">L</span> is an
estimator of λ. And not just any estimator; it is also the
maximum likelihood estimator (see
</span><a href="http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood"><span style="font-family:monospace;font-size:medium">http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood</span></a><span style="font-size:medium">).
So if you want to maximize your chance of guessing λ exactly,
<span style="font-style:italic">L</span> is the way to go.
</span><a id="hevea_default797"></a><span style="font-size:medium">
</span><a id="hevea_default798"></a></p>
<p><span style="font-size:medium">But we know that <span style="text-decoration:overline">x</span> is not robust in the presence of outliers, so
we expect <span style="font-style:italic">L</span> to have the same problem.
</span><a id="hevea_default799"></a><span style="font-size:medium">
</span><a id="hevea_default800"></a><span style="font-size:medium">
</span><a id="hevea_default801"></a></p>
<p><span style="font-size:medium">We can choose an alternative based on the sample median.
The median of an exponential distribution is ln(2) / λ,
so working backwards again, we can define an estimator
</span></p>
<table class="display dcenter"><tr style="vertical-align:middle"><td class="dcell">
<span style="font-style:italic;font-size:medium">L</span><sub><span style="font-style:italic;font-size:medium">m</span></sub><span style="font-size:medium"> = ln(2) / <span style="font-style:italic">m</span> </span>
</td></tr></table>
<p><span style="font-size:medium">
where <span style="font-style:italic">m</span> is the sample median.
</span><a id="hevea_default802"></a></p>
<p><span style="font-size:medium">To test the performance of these estimators, we can simulate the
sampling process:</span></p>
<pre class="verbatim"><span style="font-size:medium">def Estimate3(n=7, m=1000):
    lam = 2

    means = []
    medians = []
    for _ in range(m):
        xs = np.random.exponential(1.0/lam, n)
        L = 1 / np.mean(xs)
        Lm = math.log(2) / thinkstats2.Median(xs)
        means.append(L)
        medians.append(Lm)

    print('rmse L', RMSE(means, lam))
    print('rmse Lm', RMSE(medians, lam))
    print('mean error L', MeanError(means, lam))
    print('mean error Lm', MeanError(medians, lam))
</span></pre>
<p><span style="font-size:medium">When I run this experiment with λ=2, the RMSE of <span style="font-style:italic">L</span> is
1.1. For the median-based estimator <span style="font-style:italic">L</span></span><sub><span style="font-style:italic;font-size:medium">m</span></sub><span style="font-size:medium">, RMSE is 1.8. We can’t
tell from this experiment whether <span style="font-style:italic">L</span> minimizes MSE, but at least
it seems better than <span style="font-style:italic">L</span></span><sub><span style="font-style:italic;font-size:medium">m</span></sub><span style="font-size:medium">.
</span><a id="hevea_default803"></a><span style="font-size:medium">
</span><a id="hevea_default804"></a></p>
<p><span style="font-size:medium">Sadly, it seems that both estimators are biased. For <span style="font-style:italic">L</span> the mean
error is 0.33; for <span style="font-style:italic">L</span></span><sub><span style="font-style:italic;font-size:medium">m</span></sub><span style="font-size:medium"> it is 0.45. And neither converges to 0
as <span style="font-family:monospace">m</span> increases.
</span><a id="hevea_default805"></a><span style="font-size:medium">
</span><a id="hevea_default806"></a></p>
<p><span style="font-size:medium">It turns out that <span style="text-decoration:overline">x</span> is an unbiased estimator of the mean
of the distribution, 1 / λ, but <span style="font-style:italic">L</span> is not an unbiased
estimator of λ.</span></p>
<span style="font-size:medium">
</span><h2 id="sec86" class="section"><span style="font-size:medium">8.6  Exercises</span></h2>
<p><span style="font-size:medium">For the following exercises, you might want to start with a copy of
<span style="font-family:monospace">estimation.py</span>. Solutions are in <code>chap08soln.py</code></span></p>
<div class="theorem">
<span style="font-size:medium"><span style="font-weight:bold">Exercise 1</span>  </span><p><span style="font-size:medium"><em>In this chapter we used </em><span style="text-decoration:overline">x</span><em> and median to estimate </em>µ<em>, and
found that </em><span style="text-decoration:overline">x</span><em> yields lower MSE.
Also, we used </em><span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"><em> and </em><span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"><em> to estimate </em>σ<em>, and found that
</em><span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"><em> is biased and </em><span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"><em> unbiased.</em></span></p>
<p><span style="font-size:medium"><em>Run similar experiments to see if </em><span style="text-decoration:overline">x</span><em> and median are biased estimates
of </em>µ<em>.
Also check whether </em><span style="font-style:italic">S</span></span><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"><em> or </em><span style="font-style:italic">S</span></span><sub><span style="font-size:medium"><span style="font-style:italic">n</span>−1</span></sub><sup><span style="font-size:medium">2</span></sup><span style="font-size:medium"><em> yields a lower MSE.
</em></span><a id="hevea_default807"></a><span style="font-size:medium">
</span><a id="hevea_default808"></a><span style="font-size:medium">
</span><a id="hevea_default809"></a></p>
</div>
<div class="theorem">
<span style="font-size:medium"><span style="font-weight:bold">Exercise 2</span>  </span><p><span style="font-size:medium"><em>Suppose you draw a sample with size </em><span style="font-style:italic">n</span>=10<em> from 
an exponential distribution with </em>λ=2<em>. Simulate
this experiment 1000 times and plot the sampling distribution of
the estimate </em><span style="font-style:italic">L</span><em>. Compute the standard error of the estimate
and the 90% confidence interval.
</em></span><a id="hevea_default810"></a><span style="font-size:medium">
</span><a id="hevea_default811"></a><span style="font-size:medium">
</span><a id="hevea_default812"></a></p>
<p><span style="font-size:medium"><em>Repeat the experiment with a few different values of </em><span style="font-style:italic">n</span><em> and make
a plot of standard error versus </em><span style="font-style:italic">n</span><em>.
</em></span><a id="hevea_default813"></a><span style="font-size:medium">
</span><a id="hevea_default814"></a></p>
</div>
<div class="theorem">
<span style="font-size:medium"><span style="font-weight:bold">Exercise 3</span>  </span><p><span style="font-size:medium"><em>In games like hockey and soccer, the time between goals is
roughly exponential. So you could estimate a team’s goal-scoring rate
by observing the number of goals they score in a game. This
estimation process is a little different from sampling the time
between goals, so let’s see how it works.
</em></span><a id="hevea_default815"></a><span style="font-size:medium">
</span><a id="hevea_default816"></a></p>
<p><span style="font-size:medium"><em>Write a function that takes a goal-scoring rate, <span style="font-family:monospace">lam</span>, in goals
per game, and simulates a game by generating the time between goals
until the total time exceeds 1 game, then returns the number of goals
scored.</em></span></p>
<p><span style="font-size:medium"><em>Write another function that simulates many games, stores the
estimates of <span style="font-family:monospace">lam</span>, then computes their mean error and RMSE.</em></span></p>
<p><span style="font-size:medium"><em>Is this way of making an estimate biased? Plot the sampling
distribution of the estimates and the 90% confidence interval. What
is the standard error? What happens to sampling error for increasing
values of <span style="font-family:monospace">lam</span>?
</em></span><a id="hevea_default817"></a><span style="font-size:medium">
</span><a id="hevea_default818"></a><span style="font-size:medium">
</span><a id="hevea_default819"></a><span style="font-size:medium">
</span><a id="hevea_default820"></a></p>
</div>
<span style="font-size:medium">
</span><h2 id="sec87" class="section"><span style="font-size:medium">8.7  Glossary</span></h2>
<ul class="itemize">
<li class="li-itemize">
<span style="font-size:medium">estimation: The process of inferring the parameters of a distribution
from a sample.
</span><a id="hevea_default821"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">estimator: A statistic used to estimate a parameter.
</span><a id="hevea_default822"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">mean squared error (MSE): A measure of estimation error.
</span><a id="hevea_default823"></a><span style="font-size:medium">
</span><a id="hevea_default824"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">root mean squared error (RMSE): The square root of MSE,
a more meaningful representation of typical error magnitude.
</span><a id="hevea_default825"></a><span style="font-size:medium">
</span><a id="hevea_default826"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">maximum likelihood estimator (MLE): An estimator that computes the
point estimate most likely to be correct.
</span><a id="hevea_default827"></a><span style="font-size:medium">
</span><a id="hevea_default828"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">bias (of an estimator): The tendency of an estimator to be above or
below the actual value of the parameter, when averaged over repeated
experiments. </span><a id="hevea_default829"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">sampling error: Error in an estimate due to the limited
size of the sample and variation due to chance. </span><a id="hevea_default830"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">sampling bias: Error in an estimate due to a sampling process
that is not representative of the population. </span><a id="hevea_default831"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">measurement error: Error in an estimate due to inaccuracy collecting
or recording data. </span><a id="hevea_default832"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">sampling distribution: The distribution of a statistic if an
experiment is repeated many times. </span><a id="hevea_default833"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">standard error: The RMSE of an estimate,
which quantifies variability due to sampling error (but not
other sources of error).
</span><a id="hevea_default834"></a>
</li>
<li class="li-itemize">
<span style="font-size:medium">confidence interval: An interval that represents the expected
range of an estimator if an experiment is repeated many times.
</span><a id="hevea_default835"></a><span style="font-size:medium"> </span><a id="hevea_default836"></a>
</li>
</ul>
<span style="font-size:medium">
</span>
</td>