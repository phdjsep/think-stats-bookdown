\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Think Stats},
            pdfauthor={Allen B. Downey},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Think Stats}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \subtitle{Exploratory Data Analysis in Python}
  \author{Allen B. Downey}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-03-22}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

You might prefer to read the \href{http://thinkstats2.com/thinkstats2.pdf}{PDF
version}, or you can buy a
hardcopy from \href{http://amzn.to/2gBBW7v}{Amazon}.

This book is an introduction to the practical tools of exploratory data
analysis. The organization of the book follows the process I use when I
start working with a dataset:

\begin{itemize}
\tightlist
\item
  Importing and cleaning: Whatever format the data is in, it usually
  takes some time and effort to read the data, clean and transform it,
  and check that everything made it through the translation process
  intact.
\item
  Single variable explorations: I usually start by examining one
  variable at a time, finding out what the variables mean, looking at
  distributions of the values, and choosing appropriate summary
  statistics.
\item
  Pair-wise explorations: To identify possible relationships between
  variables, I look at tables and scatter plots, and compute
  correlations and linear fits.
\item
  Multivariate analysis: If there are apparent relationships between
  variables, I use multiple regression to add control variables and
  investigate more complex relationships.
\item
  Estimation and hypothesis testing: When reporting statistical
  results, it is important to answer three questions: How big is the
  effect? How much variability should we expect if we run the same
  measurement again? Is it possible that the apparent effect is due to
  chance?
\item
  Visualization: During exploration, visualization is an important
  tool for finding possible relationships and effects. Then if an
  apparent effect holds up to scrutiny, visualization is an effective
  way to communicate results.
\end{itemize}

This book takes a computational approach, which has several advantages
over mathematical approaches:

\begin{itemize}
\tightlist
\item
  I present most ideas using Python code, rather than mathematical
  notation. In general, Python code is more readable; also, because it
  is executable, readers can download it, run it, and modify it.
\item
  Each chapter includes exercises readers can do to develop and
  solidify their learning. When you write programs, you express your
  understanding in code; while you are debugging the program, you are
  also correcting your understanding.
\item
  Some exercises involve experiments to test statistical behavior. For
  example, you can explore the Central Limit Theorem (CLT) by
  generating random samples and computing their sums. The resulting
  visualizations demonstrate why the CLT works and when it doesn't.
\item
  Some ideas that are hard to grasp mathematically are easy to
  understand by simulation. For example, we approximate p-values by
  running random simulations, which reinforces the meaning of the
  p-value.
\item
  Because the book is based on a general-purpose programming language
  (Python), readers can import data from almost any source. They are
  not limited to datasets that have been cleaned and formatted for a
  particular statistics tool.
\end{itemize}

The book lends itself to a project-based approach. In my class, students
work on a semester-long project that requires them to pose a statistical
question, find a dataset that can address it, and apply each of the
techniques they learn to their own data.

To demonstrate my approach to statistical analysis, the book presents a
case study that runs through all of the chapters. It uses data from two
sources:

\begin{itemize}
\tightlist
\item
  The National Survey of Family Growth (NSFG), conducted by the U.S.
  Centers for Disease Control and Prevention (CDC) to gather
  ``information on family life, marriage and divorce, pregnancy,
  infertility, use of contraception, and men's and women's health.''
  (See \url{http://cdc.gov/nchs/nsfg.htm}.)
\item
  The Behavioral Risk Factor Surveillance System (BRFSS), conducted by
  the National Center for Chronic Disease Prevention and Health
  Promotion to ``track health conditions and risk behaviors in the
  United States.'' (See \url{http://cdc.gov/BRFSS/}.)
\end{itemize}

Other examples use data from the IRS, the U.S. Census, and the Boston
Marathon.

This second edition of Think Stats includes the chapters from the first edition, many of them substantially revised, and new chapters on regression, time series analysis, survival analysis, and analytic methods. The previous edition did not use pandas, SciPy, or StatsModels, so all of that material is new.

\hypertarget{how-i-wrote-this-book}{%
\section{How I wrote this book}\label{how-i-wrote-this-book}}

When people write a new textbook, they usually start by reading a stack
of old textbooks. As a result, most books contain the same material in
pretty much the same order.

I did not do that. In fact, I used almost no printed material while I
was writing this book, for several reasons:

\begin{itemize}
\tightlist
\item
  My goal was to explore a new approach to this material, so I didn't
  want much exposure to existing approaches.
\item
  Since I am making this book available under a free license, I wanted
  to make sure that no part of it was encumbered by copyright
  restrictions.
\item
  Many readers of my books don't have access to libraries of printed
  material, so I tried to make references to resources that are freely
  available on the Internet.
\item
  Some proponents of old media think that the exclusive use of
  electronic resources is lazy and unreliable. They might be right
  about the first part, but I think they are wrong about the second,
  so I wanted to test my theory.
\end{itemize}

The resource I used more than any other is Wikipedia. In general, the
articles I read on statistical topics were very good (although I made a
few small changes along the way). I include references to Wikipedia
pages throughout the book and I encourage you to follow those links; in
many cases, the Wikipedia page picks up where my description leaves off.
The vocabulary and notation in this book are generally consistent with
Wikipedia, unless I had a good reason to deviate. Other resources I
found useful were Wolfram MathWorld and the Reddit statistics forum,
\url{http://www.reddit.com/r/statistics}.

\hypertarget{using-the-code}{%
\section{Using the code}\label{using-the-code}}

The code and data used in this book are available from \href{https://github.com/AllenDowney/ThinkStats2}{\textgreater{}https://github.com/AllenDowney/ThinkStats2}. Git is a version control system that allows you to keep track of the files that make up a project. A collection of files under Git's control is called a repository. GitHub is
a hosting service that provides storage for Git repositories and a convenient web interface.

The GitHub homepage for my repository provides several ways to work with
the code:

\begin{itemize}
\tightlist
\item
  You can create a copy of my repository on GitHub by pressing the
  Fork button. If you don't already have a GitHub account, you'll need to create one. After forking, you'll have your own repository on GitHub that you can use to keep track of code you write while working on this book. Then you can clone the repo, which means that you make a copy of the files on your computer.
\item
  Or you could clone my repository. You don't need a GitHub account to
  do this, but you won't be able to write your changes back to GitHub.
\item
  If you don't want to use Git at all, you can download the files in a
  Zip file using the button in the lower-right corner of the GitHub
  page.
\end{itemize}

All of the code is written to work in both Python 2 and Python 3 with no
translation.

I developed this book using Anaconda from Continuum Analytics, which is
a free Python distribution that includes all the packages you'll need to
run the code (and lots more). I found Anaconda easy to install. By
default it does a user-level installation, not system-level, so you
don't need administrative privileges. And it supports both Python 2 and
Python 3. You can download Anaconda from \url{http://continuum.io/downloads}.

If you don't want to use Anaconda, you will need the following packages:

\begin{itemize}
\tightlist
\item
  pandas for representing and analyzing data, \url{http://pandas.pydata.org/};
\item
  NumPy for basic numerical computation, \url{http://www.numpy.org/};
\item
  SciPy for scientific computation including statistics, \url{http://www.scipy.org/};
\item
  StatsModels for regression and other statistical analysis, \url{http://statsmodels.sourceforge.net/}; and
\item
  matplotlib for visualization, \url{http://matplotlib.org/}.
\end{itemize}

Although these are commonly used packages, they are not included with
all Python installations, and they can be hard to install in some
environments. If you have trouble installing them, I strongly recommend
using Anaconda or one of the other Python distributions that include
these packages.

After you clone the repository or unzip the zip file, you should have a
folder called ThinkStats2/code with a file called
nsfg.py. If you run nsfg.py, it should read a data file, run some tests,
and print a message like, ``All tests passed.'' If you get import errors,
it probably means there are packages you need to install.

Most exercises use Python scripts, but some also use the IPython
notebook. If you have not used IPython notebook before, I suggest you
start with the documentation at \url{http://ipython.org/ipython-doc/stable/notebook/notebook.html}.

I wrote this book assuming that the reader is familiar with core Python,
including object-oriented features, but not pandas, NumPy, and SciPy. If
you are already familiar with these modules, you can skip a few
sections.

I assume that the reader knows basic mathematics, including logarithms,
for example, and summations. I refer to calculus concepts in a few
places, but you don't have to do any calculus.

If you have never studied statistics, I think this book is a good place
to start. And if you have taken a traditional statistics class, I hope
this book will help repair the damage.

---

Allen B. Downey is a Professor of Computer Science at the Franklin W.
Olin College of Engineering in Needham, MA.

\hypertarget{contributor-list}{%
\section{Contributor List}\label{contributor-list}}

If you have a suggestion or correction, please send email to \href{mailto:downey@allendowney.com}{\nolinkurl{downey@allendowney.com}}. If I make a
change based on your feedback, I will add you to the contributor list
(unless you ask to be omitted).

If you include at least part of the sentence the error appears in, that
makes it easy for me to search. Page and section numbers are fine, too,
but not quite as easy to work with. Thanks!

\begin{itemize}
\tightlist
\item
  Lisa Downey and June Downey read an
  early draft and made many corrections and suggestions.
\item
  Steven Zhang found several
  errors.
\item
  Andy Pethan and Molly Farison helped
  debug some of the solutions, and Molly spotted several typos.
\item
  Dr.~Nikolas Akerblom knows how big a
  Hyracotherium is.
\item
  Alex Morrow clarified one of the code
  examples.
\item
  Jonathan Street caught an error in the
  nick of time.
\item
  Many thanks to Kevin Smith and Tim
  Arnold for their work on plasTeX, which I used to convert this book
  to DocBook.
\item
  George Caplan sent several suggestions
  for improving clarity.
\item
  Julian Ceipek found an error and a
  number of typos.
\item
  Stijn Debrouwere, Leo Marihart III,
  Jonathan Hammler, and Kent Johnson found errors in the first print
  edition.
\item
  Jörg Beyer found typos in the book and
  made many corrections in the docstrings of the accompanying
  code.
\item
  Tommie Gannert sent a patch file with
  a number of corrections.
\item
  Christoph Lendenmann submitted several
  errata.
\item
  Michael Kearney sent me many excellent
  suggestions.
\item
  Alex Birch made a number of helpful
  suggestions.
\item
  Lindsey Vanderlyn, Griffin Tschurwald,
  and Ben Small read an early version of this book and found many
  errors.
\item
  John Roth, Carol Willing, and Carol
  Novitsky performed technical reviews of the book. They found many
  errors and made many helpful suggestions.
\item
  David Palmer sent many helpful
  suggestions and corrections.
\item
  Erik Kulyk found many typos.
\item
  Nir Soffer sent several excellent pull
  requests for both the book and the supporting code.
\item
  GitHub user flothesof sent a number of
  corrections.
\item
  Toshiaki Kurokawa, who is working on
  the Japanese translation of this book, has sent many corrections and
  helpful suggestions.
\item
  Benjamin White suggested more
  idiomatic Pandas code.
\item
  Takashi Sato spotted an code
  error.
\end{itemize}

Other people who found typos and similar
errors are Andrew Heine, Gábor Lipták, Dan Kearney, Alexander Gryzlov,
Martin Veillette, Haitao Ma, Jeff Pickhardt, Rohit Deshpande, Joanne
Pratt, Lucian Ursu, Paul Glezen, Ting-kuang Lin.

\hypertarget{exploratory}{%
\chapter{Exploratory Data Analysis}\label{exploratory}}

The thesis of this book is that data
combined with practical methods can answer questions and guide decisions
under uncertainty.

As an example, I present a case study
motivated by a question I heard when my wife and I were expecting our
first child: do first babies tend to arrive late?

If you Google this question, you will
find plenty of discussion. Some people claim it's true, others say it's
a myth, and some people say it's the other way around: first babies come
early.

In many of these discussions, people
provide data to support their claims. I found many examples like
these:

\begin{quote}
``My two friends that have given birth
recently to their first babies, BOTH went almost 2 weeks overdue
before going into labour or being induced.''

``My first one came 2 weeks late and now
I think the second one is going to come out two weeks early!!''

``I don't think that can be true because
my sister was my mother's first and she was early, as with many of my
cousins.''
\end{quote}

Reports like these are called anecdotal evidence because they are
based on data that is unpublished and usually personal. In casual
conversation, there is nothing wrong with anecdotes, so I don't mean to
pick on the people I quoted.

But we might want evidence that is more
persuasive and an answer that is more reliable. By those standards,
anecdotal evidence usually fails, because:

\begin{itemize}
\tightlist
\item
  Small number of observations: If
  pregnancy length is longer for first babies, the difference is
  probably small compared to natural variation. In that case, we might
  have to compare a large number of pregnancies to be sure that a
  difference exists.
\item
  Selection bias: People who join a
  discussion of this question might be interested because their first
  babies were late. In that case the process of selecting data would
  bias the results.\\
\item
  Confirmation bias: People who believe
  the claim might be more likely to contribute examples that confirm
  it. People who doubt the claim are more likely to cite
  counterexamples.\\
\item
  Inaccuracy: Anecdotes are often
  personal stories, and often misremembered, misrepresented, repeated
  inaccurately, etc.
\end{itemize}

So how can we do better?

\hypertarget{a-statistical-approach}{%
\section{A statistical approach}\label{a-statistical-approach}}

To address the limitations of anecdotes,
we will use the tools of statistics, which include:

\begin{itemize}
\item
  Data collection: We will use data
  from a large national survey that was designed explicitly with the
  goal of generating statistically valid inferences about the U.S.
  population.
\item
  Descriptive statistics: We will
  generate statistics that summarize the data concisely, and evaluate
  different ways to visualize data.
\item
  Exploratory data analysis: We will
  look for patterns, differences, and other features that address the
  questions we are interested in. At the same time we will check for
  inconsistencies and identify limitations.
\item
  Estimation: We will use data from a
  sample to estimate characteristics of the general population.
\item
  Hypothesis testing: Where we see
  apparent effects, like a difference between two groups, we will
  evaluate whether the effect might have happened by chance.
\end{itemize}

By performing these steps with care to
avoid pitfalls, we can reach conclusions that are more justifiable and
more likely to be correct.

\hypertarget{the-national-survey-of-family-growth}{%
\section{The National Survey of Family Growth}\label{the-national-survey-of-family-growth}}

Since 1973 the U.S. Centers for Disease
Control and Prevention (CDC) have conducted the National Survey of
Family Growth (NSFG), which is intended to gather ``information on family
life, marriage and divorce, pregnancy, infertility, use of
contraception, and men's and women's health. The survey results are used
\ldots{} to plan health services and health education programs, and to do
statistical studies of families, fertility, and health.'' See
\url{http://cdc.gov/nchs/nsfg.htm}.

We will use data collected by this survey
to investigate whether first babies tend to come late, and other
questions. In order to use this data effectively, we have to understand
the design of the study.

The NSFG is a cross-sectional study, which means that
it captures a snapshot of a group at a point in time. The most common
alternative is a longitudinal
study, which observes a group repeatedly over a period of time.

The NSFG has been conducted seven times;
each deployment is called a cycle.
We will use data from Cycle 6, which was conducted from January 2002 to
March 2003.

The goal of the survey is to draw
conclusions about a population;
the target population of the NSFG is people in the United States aged
15-44. Ideally surveys would collect data from every member of the
population, but that's seldom possible. Instead we collect data from a
subset of the population called a sample. The people who participate in a
survey are called respondents.

In general, cross-sectional studies are
meant to be representative, which
means that every member of the target population has an equal chance of
participating. That ideal is hard to achieve in practice, but people who
conduct surveys come as close as they can.

The NSFG is not representative; instead
it is deliberately oversampled.
The designers of the study recruited three groups---Hispanics,
African-Americans and teenagers---at rates higher than their
representation in the U.S. population, in order to make sure that the
number of respondents in each of these groups is large enough to draw
valid statistical inferences.

Of course, the drawback of oversampling
is that it is not as easy to draw conclusions about the general
population based on statistics from the survey. We will come back to
this point later.

When working with this kind of data, it
is important to be familiar with the codebook, which documents the design of
the study, the survey questions, and the encoding of the responses. The
codebook and user's guide for the NSFG data are available from
\url{http://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm}

\hypertarget{importing-the-data}{%
\section{Importing the data}\label{importing-the-data}}

The code and data used in this book are
available from \url{https://github.com/AllenDowney/ThinkStats2}. For information about downloading and working
with this code, see Section~\href{thinkstats2001.html\#code}{0.2}.

Once you download the code, you should
have a file called ThinkStats2/code/nsfg.py. If you
run it, it should read a data file, run some tests, and print a message
like, ``All tests passed.''

Let's see what it does. Pregnancy data
from Cycle 6 of the NSFG is in a file called 2002FemPreg.dat.gz; it is a
gzip-compressed data file in plain text (ASCII), with fixed width
columns. Each line in the file is a record that contains data about one
pregnancy.

The format of the file is documented in
2002FemPreg.dct, which is a
Stata dictionary file. Stata is a statistical software system; a
``dictionary'' in this context is a list of variable names, types, and
indices that identify where in each line to find each variable.

For example, here are a few lines from
2002FemPreg.dct:

\begin{verbatim}
infile dictionary {
  _column(1)  str12  caseid    %12s  "RESPONDENT ID NUMBER"
  _column(13) byte   pregordr   %2f  "PREGNANCY ORDER (NUMBER)"
}
\end{verbatim}

This dictionary describes two variables:
caseid is a 12-character
string that represents the respondent ID; pregorder is a one-byte integer
that indicates which pregnancy this record describes for this
respondent.

The code you downloaded includes thinkstats2.py, which is a Python
module that contains many classes and functions used in this book,
including functions that read the Stata dictionary and the NSFG data
file. Here's how they are used in nsfg.py:

\begin{verbatim}
def ReadFemPreg(dct_file='2002FemPreg.dct',
                dat_file='2002FemPreg.dat.gz'):
    dct = thinkstats2.ReadStataDct(dct_file)
    df = dct.ReadFixedWidth(dat_file, compression='gzip')
    CleanFemPreg(df)
    return df
\end{verbatim}

ReadStataDct takes the name of the
dictionary file and returns dct, a FixedWidthVariables object that
contains the information from the dictionary file. dct provides ReadFixedWidth, which reads the
data file.

\hypertarget{dataframes}{%
\section{DataFrames}\label{dataframes}}

The result of ReadFixedWidth is a DataFrame,
which is the fundamental data structure provided by pandas, which is a
Python data and statistics package we'll use throughout this book. A
DataFrame contains a row for each record, in this case one row per
pregnancy, and a column for each variable.

In addition to the data, a DataFrame also
contains the variable names and their types, and it provides methods for
accessing and modifying the data.

If you print df you get a truncated view of the
rows and columns, and the shape of the DataFrame, which is 13593
rows/records and 244 columns/variables.

\begin{verbatim}
>>> import nsfg
>>> df = nsfg.ReadFemPreg()
>>> df
...
[13593 rows x 244 columns]
\end{verbatim}

The DataFrame is too big to display, so
the output is truncated. The last line reports the number of rows and
columns.

The attribute columns returns a sequence of
column names as Unicode strings:

\begin{verbatim}
>>> df.columns
Index([u'caseid', u'pregordr', u'howpreg_n', u'howpreg_p', ... ])
\end{verbatim}

The result is an Index, which is another
pandas data structure. We'll learn more about Index later, but for now
we'll treat it like a list:

\begin{verbatim}
>>> df.columns[1]
'pregordr'
\end{verbatim}

To access a column from a DataFrame, you
can use the column name as a key:

\begin{verbatim}
>>> pregordr = df['pregordr']
>>> type(pregordr)
<class 'pandas.core.series.Series'>
\end{verbatim}

The result is a Series, yet another
pandas data structure. A Series is like a Python list with some
additional features. When you print a Series, you get the indices and
the corresponding values:

\begin{verbatim}
>>> pregordr
0     1
1     2
2     1
3     2
...
13590    3
13591    4
13592    5
Name: pregordr, Length: 13593, dtype: int64
\end{verbatim}

In this example the indices are integers
from 0 to 13592, but in general they can be any sortable type. The
elements are also integers, but they can be any type.

The last line includes the variable name,
Series length, and data type; int64 is one of the types provided
by NumPy. If you run this example on a 32-bit machine you might see
int32.

You can access the elements of a Series
using integer indices and slices:

\begin{verbatim}
>>> pregordr[0]
1
>>> pregordr[2:5]
2    1
3    2
4    3
Name: pregordr, dtype: int64
\end{verbatim}

The result of the index operator is an
int64; the result of the
slice is another Series.

You can also access the columns of a
DataFrame using dot notation:

\begin{verbatim}
>>> pregordr = df.pregordr
\end{verbatim}

This notation only works if the column
name is a valid Python identifier, so it has to begin with a letter,
can't contain spaces, etc.

\hypertarget{variables}{%
\section{Variables}\label{variables}}

We have already seen two variables in the
NSFG dataset, caseid and
pregordr, and we have seen
that there are 244 variables in total. For the explorations in this
book, I use the following variables:

\begin{itemize}
\item
  caseid is the integer ID of the
  respondent.
\item
  prglngth is the integer
  duration of the pregnancy in weeks.
\item
  outcome is an integer code for
  the outcome of the pregnancy. The code 1 indicates a live
  birth.
\item
  pregordr is a pregnancy serial
  number; for example, the code for a respondent's first pregnancy is
  1, for the second pregnancy is 2, and so on.
\item
  birthord is a serial number for
  live births; the code for a respondent's first child is 1, and so
  on. For outcomes other than live birth, this field is blank.
\item
  \texttt{birthwgt\_lb} and \texttt{birthwgt\_oz}
  contain the pounds and ounces parts of the birth weight of the baby.
\item
  agepreg is the mother's age at
  the end of the pregnancy.
\item
  finalwgt is the statistical
  weight associated with the respondent. It is a floating-point value
  that indicates the number of people in the U.S. population this
  respondent represents.
\end{itemize}

If you read the codebook carefully, you
will see that many of the variables are recodes, which means that they are not
part of the raw data collected by
the survey; they are calculated using the raw data.

For example, prglngth for live births is equal
to the raw variable wksgest
(weeks of gestation) if it is available; otherwise it is estimated using
mosgest * 4.33 (months of
gestation times the average number of weeks in a month).

Recodes are often based on logic that
checks the consistency and accuracy of the data. In general it is a good
idea to use recodes when they are available, unless there is a
compelling reason to process the raw data yourself.

\hypertarget{transformation}{%
\section{Transformation}\label{transformation}}

When you import data like this, you often
have to check for errors, deal with special values, convert data into
different formats, and perform calculations. These operations are called
data cleaning.

nsfg.py includes CleanFemPreg, a function that
cleans the variables I am planning to use.

\begin{verbatim}
def CleanFemPreg(df):
    df.agepreg /= 100.0

    na_vals = [97, 98, 99]
    df.birthwgt_lb.replace(na_vals, np.nan, inplace=True)
    df.birthwgt_oz.replace(na_vals, np.nan, inplace=True)

    df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0    
\end{verbatim}

agepreg contains the mother's age
at the end of the pregnancy. In the data file, agepreg is encoded as an integer
number of centiyears. So the first line divides each element of agepreg by 100, yielding a
floating-point value in years.

\texttt{birthwgt\_lb} and \texttt{birthwgt\_oz} contain
the weight of the baby, in pounds and ounces, for pregnancies that end
in live birth. In addition it uses several special codes:

\begin{verbatim}
97 NOT ASCERTAINED
98 REFUSED  
99 DON'T KNOW
\end{verbatim}

Special values encoded as numbers are
\emph{dangerous} because if they are not handled properly, they can generate
bogus results, like a 99-pound baby. The replace method replaces these
values with np.nan, a special
floating-point value that represents ``not a number.'' The inplace flag tells replace to modify the existing
Series rather than create a new one.

As part of the IEEE floating-point
standard, all mathematical operations return nan if either argument is nan:

\begin{verbatim}
>>> import numpy as np
>>> np.nan / 100.0
nan
\end{verbatim}

So computations with nan tend to do the right thing, and
most pandas functions handle nan appropriately. But dealing with
missing data will be a recurring issue.

The last line of CleanFemPreg creates a new column
\texttt{totalwgt\_lb} that combines pounds and ounces into a single quantity, in
pounds.

One important note: when you add a new
column to a DataFrame, you must use dictionary syntax, like this

\begin{verbatim}
    # CORRECT
    df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0 
\end{verbatim}

Not dot notation, like this:

\begin{verbatim}
    # WRONG!
    df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0 
\end{verbatim}

The version with dot notation adds an
attribute to the DataFrame object, but that attribute is not treated as
a new column.

\hypertarget{validation}{%
\section{Validation}\label{validation}}

When data is exported from one software
environment and imported into another, errors might be introduced. And
when you are getting familiar with a new dataset, you might interpret
data incorrectly or introduce other misunderstandings. If you take time
to validate the data, you can save time later and avoid errors.

One way to validate data is to compute
basic statistics and compare them with published results. For example,
the NSFG codebook includes tables that summarize each variable. Here is
the table for outcome, which
encodes the outcome of each pregnancy:

\begin{verbatim}
value label           Total
1 LIVE BIRTH              9148
2 INDUCED ABORTION        1862
3 STILLBIRTH               120
4 MISCARRIAGE             1921
5 ECTOPIC PREGNANCY        190
6 CURRENT PREGNANCY        352
\end{verbatim}

The Series class provides a method,
\texttt{value\_counts}, that counts the number of times each value appears. If
we select the outcome Series
from the DataFrame, we can use \texttt{value\_counts} to compare with the
published data:

\begin{verbatim}
>>> df.outcome.value_counts(sort=False)
1    9148
2    1862
3     120
4    1921
5     190
6     352
\end{verbatim}

The result of \texttt{value\_counts} is a Series;
\texttt{sort=False} doesn't sort the Series by values, so them appear in
order.

Comparing the results with the published
table, it looks like the values in outcome are correct. Similarly,
here is the published table for \texttt{birthwgt\_lb}

\begin{verbatim}
value label                  Total
. INAPPLICABLE            4449
0-5 UNDER 6 POUNDS          1125
6 6 POUNDS                2223
7 7 POUNDS                3049
8 8 POUNDS                1889
9-95 9 POUNDS OR MORE         799
\end{verbatim}

And here are the value counts:

\begin{verbatim}
>>> df.birthwgt_lb.value_counts(sort=False)
0        8
1       40
2       53
3       98
4      229
5      697
6     2223
7     3049
8     1889
9      623
10     132
11      26
12      10
13       3
14       3
15       1
51       1
\end{verbatim}

The counts for 6, 7, and 8 pounds check
out, and if you add up the counts for 0-5 and 9-95, they check out, too.
But if you look more closely, you will notice one value that has to be
an error, a 51 pound baby!

To deal with this error, I added a line
to CleanFemPreg:

\begin{verbatim}
df.loc[df.birthwgt_lb > 20, 'birthwgt_lb'] = np.nan
\end{verbatim}

This statement replaces invalid values
with np.nan. The attribute
loc provides several ways to
select rows and columns from a DataFrame. In this example, the first
expression in brackets is the row indexer; the second expression selects
the column.

The expression \texttt{df.birthwgt\_lb\ \textgreater{}\ 20}
yields a Series of type bool,
where True indicates that the condition is true. When a boolean Series
is used as an index, it selects only the elements that satisfy the
condition.

\#\#~Interpretation

To work with data effectively, you have
to think on two levels at the same time: the level of statistics and the
level of context.

As an example, let's look at the sequence
of outcomes for a few respondents. Because of the way the data files are
organized, we have to do some processing to collect the pregnancy data
for each respondent. Here's a function that does that:

\begin{verbatim}
def MakePregMap(df):
    d = defaultdict(list)
    for index, caseid in df.caseid.iteritems():
        d[caseid].append(index)
    return d
\end{verbatim}

df is the DataFrame with pregnancy
data. The iteritems method
enumerates the index (row number) and caseid for each pregnancy.

d is a dictionary that maps from
each case ID to a list of indices. If you are not familiar with defaultdict, it is in the Python
collections module. Using
d, we can look up a
respondent and get the indices of that respondent's pregnancies.

This example looks up one respondent and
prints a list of outcomes for her pregnancies:

\begin{verbatim}
>>> caseid = 10229
>>> preg_map = nsfg.MakePregMap(df)
>>> indices = preg_map[caseid]
>>> df.outcome[indices].values
[4 4 4 4 4 4 1]
\end{verbatim}

indices is the list of indices for
pregnancies corresponding to respondent 10229.

Using this list as an index into df.outcome selects the indicated
rows and yields a Series. Instead of printing the whole Series, I
selected the values
attribute, which is a NumPy array.

The outcome code 1 indicates a live birth. Code
4 indicates a miscarriage;
that is, a pregnancy that ended spontaneously, usually with no known
medical cause.

Statistically this respondent is not
unusual. Miscarriages are common and there are other respondents who
reported as many or more.

But remembering the context, this data
tells the story of a woman who was pregnant six times, each time ending
in miscarriage. Her seventh and most recent pregnancy ended in a live
birth. If we consider this data with empathy, it is natural to be moved
by the story it tells.

Each record in the NSFG dataset
represents a person who provided honest answers to many personal and
difficult questions. We can use this data to answer statistical
questions about family life, reproduction, and health. At the same time,
we have an obligation to consider the people represented by the data,
and to afford them respect and gratitude.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

Exercise~1~~ \emph{In the repository you
downloaded, you should find a file named \texttt{chap01ex.ipynb}, which is an
IPython notebook. You can launch IPython notebook from the command line
like this:}

\begin{verbatim}
$ ipython notebook &
\end{verbatim}

\emph{If IPython is installed, it should
launch a server that runs in the background and open a browser to view
the notebook. If you are not familiar with IPython, I suggest you start
at} \href{http://ipython.org/ipython-doc/stable/notebook/notebook.html}{\emph{http://ipython.org/ipython-doc/stable/notebook/notebook.html}}\emph{.}

\emph{To launch the IPython notebook server,
run:}

\begin{verbatim}
$ ipython notebook &
\end{verbatim}

\emph{It should open a new browser window, but
if not, the startup message provides a URL you can load in a browser,
usually} \href{http://localhost:8888}{\emph{http://localhost:8888}}\emph{. The new window should list the notebooks in
the repository.}

\emph{Open \texttt{chap01ex.ipynb}. Some cells are
already filled in, and you should execute them. Other cells give you
instructions for exercises you should try.}

\emph{A solution to this exercise is in
\texttt{chap01soln.ipynb}}

Exercise~2~~ \emph{In the repository you
downloaded, you should find a file named \texttt{chap01ex.py}; using this file
as a starting place, write a function that reads the respondent file,
2002FemResp.dat.gz.}

\emph{The variable pregnum is a recode that indicates
how many times each respondent has been pregnant. Print the value counts
for this variable and compare them to the published results in the NSFG
codebook.}

\emph{You can also cross-validate the
respondent and pregnancy files by comparing pregnum for each respondent with
the number of records in the pregnancy file.}

\emph{You can use nsfg.MakePregMap to make a
dictionary that maps from each caseid to a list of indices into
the pregnancy DataFrame.}

\emph{A solution to this exercise is in
\texttt{chap01soln.py}}

Exercise~3~~ \emph{The best way to learn
about statistics is to work on a project you are interested in. Is there
a question like, ``Do first babies arrive late,'' that you want to
investigate?}

\emph{Think about questions you find
personally interesting, or items of conventional wisdom, or
controversial topics, or questions that have political consequences, and
see if you can formulate a question that lends itself to statistical
inquiry.}

\emph{Look for data to help you address the
question. Governments are good sources because data from public research
is often freely available. Good places to start include} \href{http://www.data.gov/}{\emph{http://www.data.gov/}}\emph{, and} \href{http://www.science.gov/}{\emph{http://www.science.gov/}}\emph{, and in the United Kingdom,} \href{http://data.gov.uk/}{\emph{http://data.gov.uk/}}\emph{.}

\emph{Two of my favorite data sets are the
General Social Survey at} \href{http://www3.norc.org/gss+website/}{\emph{http://www3.norc.org/gss+website/}}\emph{, and the European Social Survey at}
\href{http://www.europeansocialsurvey.org/}{\emph{http://www.europeansocialsurvey.org/}}\emph{.}

\emph{If it seems like someone has already
answered your question, look closely to see whether the answer is
justified. There might be flaws in the data or the analysis that make
the conclusion unreliable. In that case you could perform a different
analysis of the same data, or look for a better source of data.}

\emph{If you find a published paper that
addresses your question, you should be able to get the raw data. Many
authors make their data available on the web, but for sensitive data you
might have to write to the authors, provide information about how you
plan to use the data, or agree to certain terms of use. Be
persistent!}

\hypertarget{glossary}{%
\section{Glossary}\label{glossary}}

\begin{itemize}
\item
  anecdotal evidence: Evidence, often
  personal, that is collected casually rather than by a well-designed
  study.
\item
  population: A group we are
  interested in studying. ``Population'' often refers to a group of
  people, but the term is used for other subjects, too.
\item
  cross-sectional study: A study that
  collects data about a population at a particular point in time.
\item
  cycle: In a repeated cross-sectional
  study, each repetition of the study is called a cycle.
\item
  longitudinal study: A study that
  follows a population over time, collecting data from the same group
  repeatedly.\\
\item
  record: In a dataset, a collection
  of information about a single person or other subject.
\item
  respondent: A person who responds to
  a survey.
\item
  sample: The subset of a population
  used to collect data.
\item
  representative: A sample is
  representative if every member of the population has the same chance
  of being in the sample.
\item
  oversampling: The technique of
  increasing the representation of a sub-population in order to avoid
  errors due to small sample sizes.
\item
  raw
  data: Values collected and recorded with little or no
  checking, calculation or interpretation.
\item
  recode: A value that is generated by
  calculation and other logic applied to raw data.
\item
  data
  cleaning: Processes that include validating data, identifying
  errors, translating between data types and representations,
  etc.
\end{itemize}

\#~Distributions \{\#distributions\}

\#\#~Histograms

One of the best ways to describe a
variable is to report the values that appear in the dataset and how many
times each value appears. This description is called the \textbf{distribution} of the variable.

The most common representation of a
distribution is a \textbf{histogram}, which
is a graph that shows the \textbf{frequency} of each value. In this
context, ``frequency'' means the number of times the value appears.

In Python, an efficient way to compute
frequencies is with a dictionary. Given a sequence of values, \texttt{t}:

\begin{verbatim}
hist = {}
for x in t:
    hist[x] = hist.get(x, 0) + 1
\end{verbatim}

The result is a dictionary that maps from
values to frequencies. Alternatively, you could use the \texttt{Counter} class defined in the \texttt{collections} module:

\begin{verbatim}
from collections import Counter
counter = Counter(t)
\end{verbatim}

The result is a \texttt{Counter} object, which is a subclass
of dictionary.

Another option is to use the pandas
method \texttt{value\_counts}, which we saw in the previous chapter. But for
this book I created a class, Hist, that represents histograms and
provides the methods that operate on them.

\#\#~Representing Histograms

The Hist constructor can take a sequence,
dictionary, pandas Series, or another Hist. You can instantiate a Hist
object like this:

\begin{verbatim}
>>> import thinkstats2
>>> hist = thinkstats2.Hist([1, 2, 2, 3, 5])
>>> hist
Hist({1: 1, 2: 2, 3: 1, 5: 1})
\end{verbatim}

Hist objects provide \texttt{Freq}, which takes a value and
returns its frequency:

\begin{verbatim}
>>> hist.Freq(2)
2
\end{verbatim}

The bracket operator does the same thing:

\begin{verbatim}
>>> hist[2]
2
\end{verbatim}

If you look up a value that has never
appeared, the frequency is 0.

\begin{verbatim}
>>> hist.Freq(4)
0
\end{verbatim}

\texttt{Values} returns an unsorted list of
the values in the Hist:

\begin{verbatim}
>>> hist.Values()
[1, 5, 3, 2]
\end{verbatim}

To loop through the values in order, you
can use the built-in function \texttt{sorted}:

\begin{verbatim}
for val in sorted(hist.Values()):
    print(val, hist.Freq(val))
\end{verbatim}

Or you can use \texttt{Items} to iterate through
value-frequency pairs:

\begin{verbatim}
for val, freq in hist.Items():
     print(val, freq)
\end{verbatim}

\#\#~Plotting Histograms

\begin{quote}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\includegraphics{thinkstats2001.png}

Figure 2.1: Histogram of the pound part of birth weight.

\protect\hypertarget{first_wgt_lb_hist}{}{}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
\end{quote}

For this book I wrote a module called
\texttt{thinkplot.py} that provides functions for plotting Hists and other objects defined in \texttt{thinkstats2.py}.
It is based on \texttt{pyplot}, which is part of the \texttt{matplotlib} package. See
Section \href{thinkstats2001.html\#code}{0.2} for information about installing \texttt{matplotlib}.

To plot \texttt{hist} with \texttt{thinkplot}, try this:

\begin{verbatim}
>>> import thinkplot
>>> thinkplot.Hist(hist)
>>> thinkplot.Show(xlabel='value', ylabel='frequency')
\end{verbatim}

You can read the documentation for \texttt{thinkplot} at \url{http://greenteapress.com/thinkstats2/thinkplot.html}

\begin{quote}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\includegraphics{thinkstats2002.png}

Figure 2.2: Histogram of the ounce part of birth weight.

\protect\hypertarget{first_wgt_oz_hist}{}{}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
\end{quote}

\hypertarget{nsfg-variables}{%
\section{NSFG Variables}\label{nsfg-variables}}

Now let's get back to the data from the
NSFG. The code in this chapter is in \texttt{first.py}. For information about
downloading and working with this code, see Section~\href{thinkstats2001.html\#code}{0.2}.

When you start working with a new
dataset, I suggest you explore the variables you are planning to use one
at a time, and a good way to start is by looking at histograms.

In Section~\href{thinkstats2002.html\#cleaning}{1.6} we transformed \texttt{agepreg} from centiyears to years,
and combined \texttt{birthwgt\_lb} and \texttt{birthwgt\_oz} into a single quantity,
\texttt{totalwgt\_lb}. In this section I use these variables to demonstrate some
features of histograms.

\begin{quote}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\includegraphics{thinkstats2003.png}

Figure 2.3: Histogram of mother's age at end of pregnancy.

\protect\hypertarget{first_agepreg_hist}{}{}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
\end{quote}

I'll start by reading the data and selecting records for live births:

\begin{verbatim}
    preg = nsfg.ReadFemPreg()
    live = preg[preg.outcome == 1]
\end{verbatim}

The expression in brackets is a boolean
Series that selects rows from the DataFrame and returns a new DataFrame.
Next I generate and plot the histogram of \texttt{birthwgt\_lb} for live births.

\begin{verbatim}
    hist = thinkstats2.Hist(live.birthwgt_lb, label='birthwgt_lb')
    thinkplot.Hist(hist)
    thinkplot.Show(xlabel='pounds', ylabel='frequency')
\end{verbatim}

When the argument passed to Hist is a pandas Series, any \texttt{nan} values
are dropped. \texttt{label} is a
string that appears in the legend when the Hist is plotted.

\begin{quote}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\includegraphics{thinkstats2004.png}

Figure 2.4: Histogram of pregnancy length in weeks.

\protect\hypertarget{first_prglngth_hist}{}{}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
\end{quote}

Figure~\protect\hyperlink{first_wgt_lb_hist}{2.1} shows the result. The most common value,
called the \textbf{mode}, is 7 pounds. The
distribution is approximately bell-shaped, which is the shape of the
\textbf{normal} distribution, also called a \textbf{Gaussian} distribution. But unlike
a true normal distribution, this distribution is asymmetric; it has a \textbf{tail} that extends farther to the
left than to the right.

Figure~\protect\hyperlink{first_wgt_oz_hist}{2.2} shows the histogram of \texttt{birthwgt\_oz}, which is
the ounces part of birth weight. In theory we expect this distribution
to be \textbf{uniform}; that is, all values
should have the same frequency. In fact, 0 is more common than the other
values, and 1 and 15 are less common, probably because respondents round
off birth weights that are close to an integer value.

Figure~\protect\hyperlink{first_agepreg_hist}{2.3} shows the histogram of \texttt{agepreg}, the mother's
age at the end of pregnancy. The mode is 21 years. The distribution is
very roughly bell-shaped, but in this case the tail extends farther to
the right than left; most mothers are in their 20s, fewer in their
30s.

Figure~\protect\hyperlink{first_prglngth_hist}{2.4} shows the histogram of \texttt{prglngth}, the length
of the pregnancy in weeks. By far the most common value is 39 weeks. The
left tail is longer than the right; early babies are common, but
pregnancies seldom go past 43 weeks, and doctors often intervene if they
do.

\#\#~Outliers

Looking at histograms, it is easy to
identify the most common values and the shape of the distribution, but
rare values are not always visible.

Before going on, it is a good idea to
check for \textbf{outliers}, which are
extreme values that might be errors in measurement and recording, or
might be accurate reports of rare events.

Hist provides methods \texttt{Largest} and \texttt{Smallest}, which take an integer
\texttt{n} and return the \texttt{n} largest or smallest values from
the histogram:

\begin{verbatim}
    for weeks, freq in hist.Smallest(10):
        print(weeks, freq)
\end{verbatim}

In the list of pregnancy lengths for live
births, the 10 lowest values are \texttt{{[}0,\ 4,\ 9,\ 13,\ 17,\ 18,\ 19,\ 20,\ 21,\ 22\textbackslash{}{]}}. Values below 10 weeks are certainly errors; the most likely
explanation is that the outcome was not coded correctly. Values higher
than 30 weeks are probably legitimate. Between 10 and 30 weeks, it is
hard to be sure; some values are probably errors, but some represent
premature babies.

On the other end of the range, the
highest values are:

\begin{verbatim}
weeks  count
43     148
44     46
45     10
46     1
47     1
48     7
50     2
\end{verbatim}

Most doctors recommend induced labor if a
pregnancy exceeds 42 weeks, so some of the longer values are surprising.
In particular, 50 weeks seems medically unlikely.

The best way to handle outliers depends
on ``domain knowledge''; that is, information about where the data come
from and what they mean. And it depends on what analysis you are
planning to perform.

In this example, the motivating question
is whether first babies tend to be early (or late). When people ask this
question, they are usually interested in full-term pregnancies, so for
this analysis I will focus on pregnancies longer than 27 weeks.

\#\#~First Babies

Now we can compare the distribution of
pregnancy lengths for first babies and others. I divided the DataFrame
of live births using \texttt{birthord}, and computed their
histograms:

\begin{verbatim}
    firsts = live[live.birthord == 1]
    others = live[live.birthord != 1]

    first_hist = thinkstats2.Hist(firsts.prglngth)
    other_hist = thinkstats2.Hist(others.prglngth)
\end{verbatim}

Then I plotted their histograms on the
same axis:

\begin{verbatim}
    width = 0.45
    thinkplot.PrePlot(2)
    thinkplot.Hist(first_hist, align='right', width=width)
    thinkplot.Hist(other_hist, align='left', width=width)
    thinkplot.Show(xlabel='weeks', ylabel='frequency',
                   xlim=[27, 46])
\end{verbatim}

\texttt{thinkplot.PrePlot} takes the number
of histograms we are planning to plot; it uses this information to
choose an appropriate collection of colors.

\begin{quote}
\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\includegraphics{thinkstats2005.png}

Figure 2.5: Histogram of pregnancy lengths.

\protect\hypertarget{first_nsfg_hist}{}{}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
\end{quote}

\texttt{thinkplot.Hist} normally uses \texttt{align=’center’} so that each bar is
centered over its value. For this figure, I use \texttt{align=’right’} and \texttt{align=’left’} to place corresponding
bars on either side of the value.

With \texttt{width=0.45}, the total width of the
two bars is 0.9, leaving some space between each pair.

Finally, I adjust the axis to show only
data between 27 and 46 weeks. Figure~\protect\hyperlink{first_nsfg_hist}{2.5} shows the result.

Histograms are useful because they make
the most frequent values immediately apparent. But they are not the best
choice for comparing two distributions. In this example, there are fewer
``first babies'' than ``others,'' so some of the apparent differences in the
histograms are due to sample sizes. In the next chapter we address this
problem using probability mass functions.

\hypertarget{summarizing-distributions}{%
\section{Summarizing Distributions}\label{summarizing-distributions}}

A histogram is a complete description of
the distribution of a sample; that is, given a histogram, we could
reconstruct the values in the sample (although not their order).

If the details of the distribution are
important, it might be necessary to present a histogram. But often we
want to summarize the distribution with a few descriptive
statistics.

Some of the characteristics we might want
to report are:

\begin{itemize}
\tightlist
\item
  central tendency: Do the values tend
  to cluster around a particular point?
\item
  modes: Is there more than one
  cluster?
\item
  spread: How much variability is there
  in the values?
\item
  tails: How quickly do the
  probabilities drop off as we move away from the modes?
\item
  outliers: Are there extreme values
  far from the modes?
\end{itemize}

Statistics designed to answer these
questions are called \textbf{summary statistics}. By far the most common summary statistic is the \textbf{mean},
which is meant to describe the central tendency of the distribution.

If you have a sample of \texttt{n} values, \emph{xi}, the mean, {x}, is the sum of the values
divided by the number of values; in other words

{x}~=~

1

{n}

~

~

{∑}

{i}

~{x}{i}~

The words ``mean'' and ``average'' are
sometimes used interchangeably, but I make this distinction:

\begin{itemize}
\tightlist
\item
  The ``mean'' of a sample is the summary
  statistic computed with the previous formula.
\item
  An ``average'' is one of several
  summary statistics you might choose to describe a central tendency.
\end{itemize}

Sometimes the mean is a good description
of a set of values. For example, apples are all pretty much the same
size (at least the ones sold in supermarkets). So if I buy 6 apples and
the total weight is 3 pounds, it would be a reasonable summary to say
they are about a half pound each.

But pumpkins are more diverse. Suppose I
grow several varieties in my garden, and one day I harvest three
decorative pumpkins that are 1 pound each, two pie pumpkins that are 3
pounds each, and one Atlantic Giant®~pumpkin that weighs 591 pounds. The
mean of this sample is 100 pounds, but if I told you ``The average
pumpkin in my garden is 100 pounds,'' that would be misleading. In this
example, there is no meaningful average because there is no typical
pumpkin.

\#\#~Variance

If there is no single number that
summarizes pumpkin weights, we can do a little better with two numbers:
mean and \textbf{variance}.

Variance is a summary statistic intended
to describe the variability or spread of a distribution. The variance of
a set of values is

{S}2~=~

1

{n}

~

~

{∑}

{i}

~({x}{i}~−~{x})2~

The term \emph{xi} − {x} is called the ``deviation from
the mean,'' so variance is the mean squared deviation. The square root of
variance, \emph{S}, is the \textbf{standard deviation}.

If you have prior experience, you might
have seen a formula for variance with \emph{n}−1 in the denominator, rather than
\texttt{n}. This statistic is used to
estimate the variance in a population using a sample. We will come back
to this in Chapter~\href{thinkstats2009.html\#estimation}{8}.

Pandas data structures provides methods
to compute mean, variance and standard deviation:

\begin{verbatim}
    mean = live.prglngth.mean()
    var = live.prglngth.var()
    std = live.prglngth.std()
\end{verbatim}

For all live births, the mean pregnancy
length is 38.6 weeks, the standard deviation is 2.7 weeks, which means
we should expect deviations of 2-3 weeks to be common.

Variance of pregnancy length is 7.3,
which is hard to interpret, especially since the units are
weeks2, or ``square weeks.'' Variance is useful in some
calculations, but it is not a good summary statistic.

\hypertarget{effect-size}{%
\section{Effect Size}\label{effect-size}}

An \textbf{effect
size} is a summary statistic intended to describe (wait for it)
the size of an effect. For example, to describe the difference between
two groups, one obvious choice is the difference in the means.

Mean pregnancy length for first babies is
38.601; for other babies it is 38.523. The difference is 0.078 weeks,
which works out to 13 hours. As a fraction of the typical pregnancy
length, this difference is about 0.2\%.

If we assume this estimate is accurate,
such a difference would have no practical consequences. In fact, without
observing a large number of pregnancies, it is unlikely that anyone
would notice this difference at all.

Another way to convey the size of the
effect is to compare the difference between groups to the variability
within groups. Cohen's \emph{d} is a
statistic intended to do that; it is defined

{d}~=~

{x\_1}~−~{x\_2}

{s}

~~

where {x\_1} and {x\_2} are the means of the groups
and \emph{s} is the ``pooled standard
deviation''. Here's the Python code that computes Cohen's \emph{d}:

\begin{verbatim}
def CohenEffectSize(group1, group2):
    diff = group1.mean() - group2.mean()

    var1 = group1.var()
    var2 = group2.var()
    n1, n2 = len(group1), len(group2)

    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)
    d = diff / math.sqrt(pooled_var)
    return d
\end{verbatim}

In this example, the difference in means
is 0.029 standard deviations, which is small. To put that in
perspective, the difference in height between men and women is about 1.7
standard deviations (see \url{https://en.wikipedia.org/wiki/Effect_size}).

\#\#~Reporting Results

We have seen several ways to describe the
difference in pregnancy length (if there is one) between first babies
and others. How should we report these results?

The answer depends on who is asking the
question. A scientist might be interested in any (real) effect, no
matter how small. A doctor might only care about effects that are \textbf{clinically significant}; that is,
differences that affect treatment decisions. A pregnant woman might be
interested in results that are relevant to her, like the probability of
delivering early or late.

How you report results also depends on
your goals. If you are trying to demonstrate the importance of an
effect, you might choose summary statistics that emphasize differences.
If you are trying to reassure a patient, you might choose statistics
that put the differences in context.

Of course your decisions should also be
guided by professional ethics. It's ok to be persuasive; you \emph{should}
design statistical reports and visualizations that tell a story clearly.
But you should also do your best to make your reports honest, and to
acknowledge uncertainty and limitations.

\#\#~Exercises

\textbf{Exercise~1}~~
Based on the results in this chapter, suppose you were asked to summarize what you learned about
whether first babies arrive late.

Which summary statistics would you use
if you wanted to get a story on the evening news? Which ones would you
use if you wanted to reassure an anxious patient?

Finally, imagine that you are Cecil Adams, author of \emph{The Straight
Dope} (\url{http://straightdope.com}), and your job is to answer the question, ``Do
first babies arrive late?'' Write a paragraph that uses the results in
this chapter to answer the question clearly, precisely, and honestly.

\textbf{Exercise~2}~~
In the repository you downloaded, you should find a file named \texttt{chap02ex.ipynb}; open it. Some
cells are already filled in, and you should execute them. Other cells
give you instructions for exercises. Follow the instructions and fill in
the answers.

\emph{A solution to this exercise is in \texttt{chap02soln.ipynb}}

In the repository you downloaded, you
should find a file named \texttt{chap02ex.py}; you can use this file as a
starting place for the following exercises. My solution is in
\texttt{chap02soln.py}.

\textbf{Exercise~3}~~
The mode of a distribution is the most frequent value; see \url{http://wikipedia.org/wiki/Mode_(statistics)}. Write a function called \texttt{Mode} that takes a Hist and returns
the most frequent value.

As a more challenging exercise, write a function called \texttt{AllModes} that
returns a list of value-frequency pairs in descending order of
frequency.

\textbf{Exercise~4}~~
Using the variable \texttt{totalwgt\_lb}, investigate whether first babies are lighter or heavier
than others. Compute Cohen's \emph{d} to quantify the difference between the groups. How does it compare to
the difference in pregnancy length?

\hypertarget{glossary-1}{%
\section{Glossary}\label{glossary-1}}

\begin{itemize}
\tightlist
\item
  distribution: The values that appear
  in a sample and the frequency of each.
\item
  histogram: A mapping from values to
  frequencies, or a graph that shows this mapping.
\item
  frequency: The number of times a
  value appears in a sample.
\item
  mode: The most frequent value in a
  sample, or one of the most frequent values.
\item
  normal distribution: An idealization
  of a bell-shaped distribution; also known as a Gaussian
  distribution.
\item
  uniform distribution: A distribution
  in which all values have the same frequency.
\item
  tail: The part of a distribution at
  the high and low extremes.
\item
  central tendency: A characteristic of
  a sample or population; intuitively, it is an average or typical
  value.
\item
  outlier: A value far from the central
  tendency.
\item
  spread: A measure of how spread out
  the values in a distribution are.
\item
  summary statistic: A statistic that
  quantifies some aspect of a distribution, like central tendency or
  spread.
\item
  variance: A summary statistic often
  used to quantify spread.
\item
  standard deviation: The square root
  of variance, also used as a measure of spread.
\item
  effect size: A summary statistic
  intended to quantify the size of an effect like a difference between
  groups.
\item
  clinically significant: A result,
  like a difference between groups, that is relevant in practice.
\end{itemize}


\end{document}
